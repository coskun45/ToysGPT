{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.ytimg.com/vi/aircAruvnKk/hqdefault.jpg](https://i.ytimg.com/vi/aircAruvnKk/hqdefault.jpg)\n",
    "\n",
    "[Neural networks (3Blue1Brown) · Course](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=Z4aX4QoYJJe9gOiw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://i.ytimg.com/vi/l8pRSuU81PU/hqdefault.jpg](https://i.ytimg.com/vi/l8pRSuU81PU/hqdefault.jpg)\n",
    "\n",
    "[Let's reproduce GPT-2 (124M)](https://youtu.be/l8pRSuU81PU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transormer Mimarisi](transformer_mimari.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention Blok Şeması](attention_blok_semasi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # Matematiksel fonksiyonlar ve sabitler için kullanılır.\n",
    "from dataclasses import dataclass  # Veri sınıflarını basit ve anlaşılır bir şekilde tanımlamak için kullanılır.\n",
    "\n",
    "import torch  # PyTorch kütüphanesi, makine öğrenmesi ve yapay zeka modelleri için kullanılır.\n",
    "import torch.nn as nn  # PyTorch'un sinir ağı (neural network) modüllerini içerir.\n",
    "from torch.nn import functional as F  # PyTorch'un sinir ağı işlevselliklerini içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # tüm başlıklar için key, query, value projeksiyonları, ancak toplu olarak\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # çıktı projeksiyonu\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # düzenleme\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # aslında bir 'bias' değil, daha çok bir maske, ancak OpenAI/HF adlandırmasını takip ediyor\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x, print_info=False):\n",
    "        # x'in şekli [1, 8, 768]\n",
    "        if print_info:\n",
    "            print(\"B, T, C = x.size() = \", x.size())\n",
    "        B, T, C = x.size()  # batch, sıra uzunluğu, gömme boyutu (n_embd)\n",
    "                            # C, transformerdeki kanal sayısıdır, yani kanal sayısı gömme boyutuna eşittir.\n",
    "        # tüm başlıklar için query, key, value hesaplayın ve başlığı ileriye taşıyın\n",
    "        # nh \"başlık sayısı\"dır, hs \"başlık boyutu\"dur ve C (kanal sayısı) = nh * hs'dir\n",
    "        # örneğin GPT-2 (124M) modelinde, n_head=12, hs=64, bu nedenle nh * hs=C=768 Kanal transformerde\n",
    "        qkv = self.c_attn(x)\n",
    "        if print_info:\n",
    "            print(\"qkv.size() = \", qkv.size()) # qkv.size() =  torch.Size([1, 8, 2304])\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        if print_info:\n",
    "            print(\"q.size() = \", q.size()) # q.size() =  torch.Size([1, 8, 768])\n",
    "            print(\"k.size() = \", k.size()) # k.size() =  torch.Size([1, 8, 768])\n",
    "            print(\"v.size() = \", v.size()) # v.size() =  torch.Size([1, 8, 768])\n",
    "        # q, k, v = [B, T, C] -> [B, T, 3, nh, hs] -> [B, nh, T, hs]\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        if print_info:\n",
    "            print(\"q.size() = \", q.size()) # q.size() =  torch.Size([1, 12, 8, 64])\n",
    "            print(\"k.size() = \", k.size()) # k.size() =  torch.Size([1, 12, 8, 64])\n",
    "            print(\"v.size() = \", v.size()) # v.size() =  torch.Size([1, 12, 8, 64])\n",
    "        # bu, tüm query ve key'ler için büyük (T,T) matrisini oluşturur\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "                                                                        # (1, 12, 8, 64) x (1, 12, 64, 8) -> (1, 12, 8, 8)\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # matrisi maskelemek için üst üçgeni -inf ile doldur\n",
    "                                                                        # modelin gelecekteki token'lara dikkat etmesini önlemek için\n",
    "        att = F.softmax(att, dim=-1) # (B, nh, T, T) son boyutta softmax\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # tüm başlık çıktıları yan yana yeniden birleştirilir\n",
    "        # çıktı projeksiyonu\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)  # Tam bağlantılı katman, girişten çıkışa lineer dönüşüm\n",
    "        self.gelu    = nn.GELU(approximate='tanh')  # Aktivasyon fonksiyonu olarak GELU (Gaussian Error Linear Unit)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)  # Çıkış projeksiyonu, giriş boyutunu eski haline getirir\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)  # İlk tam bağlantılı katmandan geçiş\n",
    "        x = self.gelu(x)  # GELU aktivasyon fonksiyonunu uygula\n",
    "        x = self.c_proj(x)  # Çıkış projeksiyonu katmanından geçiş\n",
    "        return x  # Çıktıyı döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)  # Birinci Katman Normu\n",
    "        self.attn = CausalSelfAttention(config)  # Nedensel Kendine Dikkat Katmanı\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)  # İkinci Katman Normu\n",
    "        self.mlp = MLP(config)  # Çok Katmanlı Algılayıcı Katmanı\n",
    "\n",
    "    def forward(self, x, print_info=False):\n",
    "        ln_1_x = self.ln_1(x)  # İlk Katman Normunu uygula\n",
    "        if print_info:\n",
    "            print(\"LayerNorm ln_1_x\", ln_1_x.shape, ln_1_x)  # İlk Katman Normunun çıktısını yazdır\n",
    "        x = x + self.attn(ln_1_x)  # Dikkat katmanından gelen çıktıyı girişe ekle\n",
    "        x = x + self.mlp(self.ln_2(x))  # İkinci Katman Normunu uygula ve MLP katmanından gelen çıktıyı ekle\n",
    "        return x  # Çıktıyı döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @dataclass\\nclass GPTConfig:\\n    block_size: int = 1024\\n    vocab_size: int = 50257\\n    n_layer: int = 12\\n    n_head: int = 12\\n    n_embd: int = 768 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown'daki bazı ufak düzeltmeler yaparak metni yeniden düzenledim:\n",
    "\n",
    "GPT modelinin parametre sayısını hesaplamak için aşağıdaki formüller kullanılır:\n",
    "\n",
    "1. **Embedding katmanı**: \\( vocab\\_size \\times n\\_embd \\)\n",
    "2. **Transformer bloğu**: Her bir Transformer bloğunda toplam parametre sayısı şu şekildedir:\n",
    "   - **LayerNorm**: İki LayerNorm katmanı vardır, her biri \\( 2 \\times n\\_embd \\)\n",
    "   - **Dikkat (Q, K, V)**: \\( 3 \\times (n\\_embd \\times (n\\_embd // n\\_head) + n\\_embd) \\)\n",
    "   - **Dikkat çıktı projeksiyonu**: \\( n\\_embd \\times n\\_embd + n\\_embd \\)\n",
    "   - **MLP (ara katmanlar)**: \\( 2 \\times (n\\_embd \\times 4 \\times n\\_embd + 4 \\times n\\_embd) \\)\n",
    "\n",
    "Bu formülleri kullanarak parametre sayısını hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Katmanı\n",
    "\n",
    "\\[\n",
    "50257 \\times 768 = 38,609,856\n",
    "\\]\n",
    "\n",
    "#### Transformer Bloğu\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 768 = 1,536\n",
    "  \\]\n",
    "\n",
    "- **Dikkat (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (768 \\times (768 // 12) + 768) = 3 \\times (768 \\times 64 + 768) = 3 \\times (49,152 + 768) = 3 \\times 49,920 = 149,760\n",
    "  \\]\n",
    "\n",
    "- **Dikkat çıktı projeksiyonu**:\n",
    "  \\[\n",
    "  768 \\times 768 + 768 = 590,592 + 768 = 591,360\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (768 \\times 4 \\times 768 + 4 \\times 768) = 2 \\times (3,145,728 + 3,072) = 2 \\times 3,148,800 = 6,297,600\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "1,536 + 149,760 + 591,360 + 6,297,600 = 7,040,256\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 7,040,256 = 84,483,072\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "38,609,856 (Embedding) + 84,483,072 (Transformer blokları) = 123,092,928\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 124 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısının 124M olduğu, verilen konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 64  # Blok boyutu\n",
    "    vocab_size: int = 32  # Kelime hazinesi boyutu\n",
    "    n_layer: int = 12     # Katman sayısı\n",
    "    n_head: int = 12      # Başlık sayısı\n",
    "    n_embd: int = 144     # Gömme boyutu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu konfigürasyon değerlerine göre modelin parametre sayısını yeniden hesaplayalım.\n",
    "\n",
    "### Hesaplamalar\n",
    "\n",
    "#### Embedding Katmanı\n",
    "\n",
    "\\[\n",
    "32 \\times 144 = 4,608\n",
    "\\]\n",
    "\n",
    "#### Transformer Bloğu\n",
    "\n",
    "Her bir Transformer bloğunda:\n",
    "\n",
    "- **LayerNorm**: \n",
    "  \\[\n",
    "  2 \\times 144 = 288\n",
    "  \\]\n",
    "\n",
    "- **Dikkat (Q, K, V)**:\n",
    "  \\[\n",
    "  3 \\times (144 \\times (144 // 12) + 144) = 3 \\times (144 \\times 12 + 144) = 3 \\times (1,728 + 144) = 3 \\times 1,872 = 5,616\n",
    "  \\]\n",
    "\n",
    "- **Dikkat çıktı projeksiyonu**:\n",
    "  \\[\n",
    "  144 \\times 144 + 144 = 20,736 + 144 = 20,880\n",
    "  \\]\n",
    "\n",
    "- **MLP**:\n",
    "  \\[\n",
    "  2 \\times (144 \\times 4 \\times 144 + 4 \\times 144) = 2 \\times (82,944 + 576) = 2 \\times 83,520 = 167,040\n",
    "  \\]\n",
    "\n",
    "Her bir Transformer bloğu için toplam:\n",
    "\\[\n",
    "288 + 5,616 + 20,880 + 167,040 = 193,824\n",
    "\\]\n",
    "\n",
    "#### Toplam Transformer Bloğu Parametreleri\n",
    "\\[\n",
    "12 \\times 193,824 = 2,325,888\n",
    "\\]\n",
    "\n",
    "#### Toplam Parametreler\n",
    "\\[\n",
    "4,608 (Embedding) + 2,325,888 (Transformer blokları) = 2,330,496\n",
    "\\]\n",
    "\n",
    "Bu hesaplamalar, yaklaşık 2.33 milyon parametreye oldukça yakındır. Modelin toplam parametre sayısı bu konfigürasyon değerlerine göre bu şekilde hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def print0(*args, **kwargs):\n",
    "    # sadece master işlemden yazdıran değiştirilmiş print fonksiyonu\n",
    "    # eğer bu dağıtılmış bir çalışma değilse, sadece normal print işlevi gibi çalışır\n",
    "    if int(os.environ.get(\"RANK\", 0)) == 0:\n",
    "        print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # wte: kelime token gömmeleri\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # wpe: pozisyon gömmeleri\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # h: transformer blokları\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # ln_f: çıktıdan önce son layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # lm_head: dil modelleme başlığı\n",
    "        self.lm_head.LLMC_SKIP_INIT = 1 # bu kısmı başlatma, ağırlıkları bağlayacağız\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # tüm ağırlıkları başlat, çok dikkatli olmak için bir torch rng nesnesi kullan\n",
    "        self.init_rng = torch.Generator()\n",
    "        self.init_rng.manual_seed(42)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # GPT-2 makalesine göre rezidüel projeksiyonlara özel ölçeklenmiş başlatma uygula\n",
    "            std = 0.02 if not hasattr(module, 'LLMC_RESIDUAL_SCALE_FLAG') else 0.02 / math.sqrt(2 * self.config.n_layer)\n",
    "            # lm_head'i başlatmaktan kaçınmak istiyoruz, çünkü wte ile paylaşılan parametreler\n",
    "            # ve wte Embedding başlatması sırasında zaten başlatıldı\n",
    "            if not hasattr(module, 'LLMC_SKIP_INIT'):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=std, generator=self.init_rng)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02, generator=self.init_rng)\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Bu uzunluktaki bir diziyi ilerletemezsiniz: {t}, blok boyutu yalnızca {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # şekil (t)\n",
    "\n",
    "        # GPT modelini ileri geçir\n",
    "        tok_emb = self.transformer.wte(idx) # token gömmeleri şekli (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # pozisyon gömmeleri şekli (t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # eğer istenen hedefler verilmişse, kaybı da hesapla\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # çıkarım zamanında mini optimizasyon: lm_head'i sadece son pozisyonda ileri geçir\n",
    "            logits = self.lm_head(x[:, [-1], :]) # zaman boyutunu korumak için liste [-1] kullanılıyor\n",
    "            loss = None\n",
    "\n",
    "        # performans nedenlerinden dolayı, eğer gerekli değilse logits'i döndürmemek uygun olur\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type, zero_stage):\n",
    "        # aday parametrelerle başla\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # grad gerektirmeyenleri filtrele\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # optimizasyon grupları oluştur. 2D olan tüm parametreler weight decay'e tabi tutulur, diğerleri tutulmaz.\n",
    "        # yani, matmul'lerdeki ve gömmelerdeki tüm ağırlık tensörleri decay edilir, tüm bias'lar ve layernorm'lar edilmez.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print0(f\"decay edilen parametre tensörlerinin sayısı: {len(decay_params)}, {num_decay_params:,} parametre\")\n",
    "        print0(f\"decay edilmeyen parametre tensörlerinin sayısı: {len(nodecay_params)}, {num_nodecay_params:,} parametre\")\n",
    "        # AdamW optimizasyonu oluştur ve mümkünse birleşik versiyonu kullan\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        print0(f\"birleşik AdamW kullanılıyor: {use_fused}\")\n",
    "        if zero_stage == 1:\n",
    "            print0(\"ZeroRedundancyOptimizer kullanılıyor\")\n",
    "            optimizer = ZeroRedundancyOptimizer(**optim_groups[0], optimizer_class=torch.optim.AdamW,\n",
    "                                                lr=learning_rate, betas=betas, fused=use_fused)\n",
    "            optimizer.add_param_group(optim_groups[1])\n",
    "        else:\n",
    "            print0(\"normal AdamW kullanılıyor\")\n",
    "            optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        idx (LongTensor şekli (b,t)) dizininin tamamlanmış halini al ve her seferinde tahminleri modele geri besle.\n",
    "        Büyük olasılıkla model.eval() modunda çalışmak isteyeceksiniz.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # dizinin bağlamı çok uzarsa, blok boyutunda kesmemiz gerekir\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # dizideki indekse ait logits'leri almak için modeli ileri geçir\n",
    "            logits, _ = self(idx_cond)\n",
    "            # logits'leri son adımda alın ve istenen sıcaklıkla ölçeklendirin\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # isteğe bağlı olarak logits'leri sadece top k seçenekleriyle kırpın\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # logits'leri (normalize edilmiş) olasılıklara dönüştürmek için softmax uygulayın\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # dağılımdan örnekleme yap\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # örneklenen indeksi devam eden diziye ekle ve devam et\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    \"\"\"\n",
    "    Dosyanın sadece başlık kısmını okur ve başlık verilerini döndürür.\n",
    "    Burada başlık, dosyanın tamamını okuduğumuz ve byte olarak depoladığımız veri olarak ele alınmıştır.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Okunacak dosyanın adı.\n",
    "\n",
    "    Returns:\n",
    "        int: Dosyadaki byte sayısı.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()  # Dosyayı byte olarak oku\n",
    "        tokens = [x for x in tokens]  # Byte'ları listeye dönüştür\n",
    "    return len(tokens)  # Byte'ların sayısını döndür\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    \"\"\"\n",
    "    Dosyayı okur ve içindeki tüm verileri byte olarak döndürür.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Okunacak dosyanın adı.\n",
    "\n",
    "    Returns:\n",
    "        list: Dosyadaki byte'ların listesi.\n",
    "    \"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        tokens = f.read()  # Dosyayı byte olarak oku\n",
    "        tokens = [x for x in tokens]  # Byte'ları listeye dönüştür\n",
    "    return tokens  # Byte'ların listesini döndür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "class DistributedDataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        \"\"\"\n",
    "        Dağıtık veri yükleyici başlatılır.\n",
    "        \n",
    "        Args:\n",
    "            filename_pattern (str): Yüklenmesi gereken dosyaların desenini belirten bir dize.\n",
    "            B (int): Batch boyutu.\n",
    "            T (int): Sequence uzunluğu.\n",
    "            process_rank (int): İşlem sırası.\n",
    "            num_processes (int): Toplam işlem sayısı.\n",
    "        \"\"\"\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # Desene uyan dosyaları bul ve sırala\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"Desene uyan hiçbir dosya bulunamadı: {filename_pattern}\"\n",
    "\n",
    "        # Tüm veri parçalarını yükle ve doğrula, toplam token sayısını say\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1, f\"Veri parçacığı {fname} mevcut ayar için çok küçük\"\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print0(f\"Veri Yükleyici: Toplam token sayısı: {ntok_total:,} {len(self.files)} dosyada\")\n",
    "\n",
    "        # Başlatma işlemi\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Veri yükleyiciyi yeniden başlatır.\n",
    "        \"\"\"\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): \n",
    "        \"\"\"\n",
    "        Sonraki veri parçasına ilerler.\n",
    "        \"\"\"\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Bir sonraki batch'i döndürür.\n",
    "        \n",
    "        Returns:\n",
    "            x (torch.Tensor): Giriş tensoru.\n",
    "            y (torch.Tensor): Hedef tensoru.\n",
    "        \"\"\"\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        buf = torch.tensor(buf, dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # Girişler\n",
    "        y = (buf[1:]).view(B, T) # Hedefler\n",
    "        # Mevcut parçacıkta başlangıç işaretçisini ilerlet\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # Eğer bir sonraki batch'i yüklemek sınırları aşarsa, parçacığı ilerlet\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fp16(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in float16 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.float16)  # Convert tensor to float16\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file\n",
    "\n",
    "def write_fp32(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in float32 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.float32)  # Convert tensor to float32\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file\n",
    "\n",
    "def write_bf16(tensor, file):\n",
    "    \"\"\"\n",
    "    Writes a tensor to a file in bfloat16 format.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: Tensor to be written.\n",
    "    - file: File object to write to.\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu().to(torch.bfloat16)  # Convert tensor to bfloat16\n",
    "    t = t.view(torch.int16)  # Trick: reinterpret as int16 for numpy compatibility\n",
    "    b = t.numpy().tobytes()  # Convert tensor to numpy array and then to bytes\n",
    "    file.write(b)  # Write bytes to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensors(model_tensors, L, file, dtype):\n",
    "    \"\"\"\n",
    "    GPT-2 modelinin ağırlıklarını bir ikili dosyaya yazar.\n",
    "\n",
    "    Parametreler:\n",
    "    - model_tensors: Model ağırlıklarının tensorlerini içeren sözlük.\n",
    "    - L: Transformer modelindeki katman sayısı.\n",
    "    - file: Tensorlerin yazılacağı dosya nesnesi.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float16', 'float32', 'bfloat16').\n",
    "\n",
    "    Notlar:\n",
    "    - 'dtype' parametresine bağlı olarak, uygun yazma fonksiyonunu ('write_fp16', 'write_fp32', 'write_bf16') seçer ve tensorleri dönüştürerek dosyaya yazar.\n",
    "    - GPT-2 modelinin çeşitli bileşenleri için tensorler yazılır; bunlar arasında gömme katmanları, katman normalleştirme ağırlıkları, dikkat ağırlıkları ve sapmaları, MLP ağırlıkları ve sapmaları, ve son katman normalleştirme ağırlıkları bulunur.\n",
    "    \"\"\"\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "\n",
    "    # 'dtype' parametresine göre uygun yazma fonksiyonunu seç\n",
    "    write_fun = write_fp16 if dtype == \"float16\" else write_fp32 if dtype == \"float32\" else write_bf16\n",
    "\n",
    "    # Model tensorlerini dosyaya yaz\n",
    "    write_fun(model_tensors[\"transformer.wte.weight\"], file)  # (V, C)\n",
    "    write_fun(model_tensors[\"transformer.wpe.weight\"], file)  # (T, C)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_1.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 3C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 3C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_attn.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.attn.c_proj.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.ln_2.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 4C, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_fc.bias\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C, 4C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.weight\"], file)\n",
    "\n",
    "    for i in range(L):  # (L, C)\n",
    "        write_fun(model_tensors[f\"transformer.h.{i}.mlp.c_proj.bias\"], file)\n",
    "\n",
    "    write_fun(model_tensors[\"transformer.ln_f.weight\"], file)  # (C, )\n",
    "    write_fun(model_tensors[\"transformer.ln_f.bias\"], file)  # (C, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pad_vocab(tensor, multiple=128, value=0):\n",
    "    \"\"\"\n",
    "    Tensorin kelime dağarcığı boyutunu en yakın uygun katına kadar doldurur.\n",
    "\n",
    "    Parametreler:\n",
    "    - tensor: (V, C) şeklinde olan giriş tensorü, burada V orijinal kelime dağarcığı boyutunu ve C özellik sayısını belirtir.\n",
    "    - multiple: Kelime dağarcığının doldurulacağı uygun kat.\n",
    "    - value: Dolgu satırlarının doldurulacağı değer.\n",
    "\n",
    "    Dönüş:\n",
    "    - padded: Şekli (Vp, C) olan tensor, burada Vp doldurulmuş kelime dağarcığı boyutunu temsil eder.\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon tensorün kelime dağarcığı boyutunu (V) 'multiple' parametresi ile belirtilen en yakın katına kadar doldurur.\n",
    "    - Bu doldurma işlemi, tensor işlemlerini özellikle GPU üzerinde daha verimli hale getirmek için yapılır.\n",
    "    - Algoritmik olarak, bu işlem tensorün anlamını değiştirmez, yalnızca boyutlarını optimize eder.\n",
    "    \"\"\"\n",
    "    assert tensor.ndim == 2  # Tensorün 2 boyutlu olduğundan emin ol (V, C)\n",
    "    V, C = tensor.shape\n",
    "\n",
    "    # Doldurulmuş kelime dağarcığı boyutunu 'multiple' ile belirtilen en yakın katına yuvarla\n",
    "    Vp = ((V + multiple - 1) // multiple) * multiple\n",
    "\n",
    "    # Gerekirse tensorü doldur\n",
    "    pad_rows = Vp - V\n",
    "    padded = tensor if pad_rows == 0 else F.pad(tensor, (0, 0, 0, pad_rows), value=value)\n",
    "\n",
    "    assert padded.shape == (Vp, C)  # Doldurulmuş tensorün doğru şekilde olduğundan emin ol (Vp, C)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model, filename, dtype):\n",
    "    \"\"\"\n",
    "    Modeli belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - model: Kaydedilecek model.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float16', 'float32', 'bfloat16').\n",
    "\n",
    "    Notlar:\n",
    "    - 'dtype' parametresi, geçerli bir veri tipi olmalıdır ('float16', 'float32', 'bfloat16').\n",
    "    - Belirtilen 'dtype' parametresine göre versiyon seçilir ve header oluşturulur.\n",
    "    - Model parametreleri header'ın ardından dosyaya yazılır, önce kelime dağarcığı boyutu 'pad_vocab' fonksiyonuyla uygun bir katına kadar doldurulur.\n",
    "    \"\"\"\n",
    "    assert dtype in {\"float16\", \"float32\", \"bfloat16\"}\n",
    "\n",
    "    # Versiyon numarasını belirle\n",
    "    version = {\n",
    "        \"float16\": 2,  # 2: Tüm tensorler float16, doldurulmuş kelime dağarcığı\n",
    "        \"float32\": 3,  # 3: Tüm tensorler float32, doldurulmuş kelime dağarcığı\n",
    "        \"bfloat16\": 5,  # 5: Tüm tensorler bfloat16, doldurulmuş kelime dağarcığı\n",
    "    }[dtype]\n",
    "\n",
    "    # Header oluştur\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240326  # Magic sayı\n",
    "    header[1] = version  # Kontrol noktası versiyonu\n",
    "    header[2] = model.config.block_size\n",
    "    header[3] = model.config.vocab_size\n",
    "    header[4] = model.config.n_layer\n",
    "    header[5] = model.config.n_head\n",
    "    header[6] = model.config.n_embd\n",
    "\n",
    "    # Parametreleri al\n",
    "    params = {name: param.cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    # Kelime dağarcığını 128'in katına kadar doldur\n",
    "    wte = params[\"transformer.wte.weight\"]  # (V, C)\n",
    "    wte_padded = pad_vocab(wte)  # (Vp, C)\n",
    "    params[\"transformer.wte.weight\"] = wte_padded  # (Vp, C)\n",
    "    print(f\"{wte.size(0)} boyutundan {wte_padded.size(0)} boyutlu doldurulmuş kelime dağarcığı\")\n",
    "    header[7] = wte_padded.size(0)  # Header'da doldurulmuş kelime dağarcığı boyutunu sakla\n",
    "\n",
    "    # Şimdi dosyaya yaz\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())  # Header'ı yaz\n",
    "        write_tensors(params, model.config.n_layer, file, dtype)  # Parametreleri yaz\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_state(model, x, y, logits, loss, filename, dtype=\"float32\"):\n",
    "    \"\"\"\n",
    "    Model durumunu belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - model: Kaydedilecek model.\n",
    "    - x: Modelin girdisi olan tensor.\n",
    "    - y: Modelin hedef çıktısı olan tensor.\n",
    "    - logits: Modelin çıktıları (logitler).\n",
    "    - loss: Modelin kaybı (loss), tek bir float değeri.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "    - dtype: Tensorleri dönüştürmek için veri tipi ('float32' varsayılan olarak).\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon, hata ayıklama için girdi, logitler, kayıp ve parametre gradyanlarını içeren bir durum bilgisi içerir.\n",
    "    - 'dtype' parametresi, geçerli bir veri tipi olmalıdır ('float32', 'float16', 'bfloat16').\n",
    "    - Header oluşturulur ve dosyaya yazılır.\n",
    "    - Girdi (x), hedef (y), logitler, kayıp (loss) ve gradyanlar dosyaya yazılır.\n",
    "    \"\"\"\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240327  # Magic sayı\n",
    "    header[1] = 2  # Çalışma durumu versiyonu = 2 (1 -> 2 doldurulmuş kelime dağarcığı değişiklikleri için)\n",
    "    header[2] = x.size(0)  # Batch boyutu B\n",
    "    header[3] = x.size(1)  # Batch'ın zamansal uzunluğu T\n",
    "\n",
    "    # Parametre gradyanlarını al\n",
    "    grads = {name: param.grad.cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    # Kelime dağarcığını gradyanlarda da uygun katına kadar doldur\n",
    "    wte_grad = grads[\"transformer.wte.weight\"]  # (V, C)\n",
    "    wte_grad_padded = pad_vocab(wte_grad, value=0)  # (Vp, C)\n",
    "    grads[\"transformer.wte.weight\"] = wte_grad_padded  # (Vp, C)\n",
    "    print(f\"Referans gradyanlarda doldurulmuş kelime dağarcığı boyutu {wte_grad.size(0)} boyutundan {wte_grad_padded.size(0)} boyutuna\")\n",
    "\n",
    "    # Dosyaya yaz\n",
    "    with open(filename, \"wb\") as file:\n",
    "        # Header\n",
    "        file.write(header.numpy().tobytes())\n",
    "        # Girdi x\n",
    "        file.write(x.cpu().numpy().astype(\"int32\").tobytes())  # (B, T)\n",
    "        # Hedef y\n",
    "        file.write(y.cpu().numpy().astype(\"int32\").tobytes())  # (B, T)\n",
    "        # Logitler (model ileri geçişinin sonucu)\n",
    "        write_fp32(logits.cpu(), file)\n",
    "        # Kayıp (çapraz entropi kaybının tek bir float değeri)\n",
    "        write_fp32(loss.cpu(), file)\n",
    "        # Gradyanlar\n",
    "        write_tensors(grads, model.config.n_layer, file, dtype)\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenizer(enc, filename):\n",
    "    \"\"\"\n",
    "    Tokenizer'ı belirtilen dosyaya yazan fonksiyon.\n",
    "\n",
    "    Parametreler:\n",
    "    - enc: Encoder (kodlayıcı) nesnesi.\n",
    "    - filename: Kaydedilecek dosyanın adı.\n",
    "\n",
    "    Notlar:\n",
    "    - Bu fonksiyon, tokenizer'ın versiyonunu, token sayısını ve EOT (End-Of-Text) token'ını içeren bir başlık oluşturur ve dosyaya yazar.\n",
    "    - Her bir token için uzunluğunu ve gerçek baytlarını bir dosyaya yazar.\n",
    "    \"\"\"\n",
    "    n = enc.max_token_value + 1\n",
    "    header = torch.zeros(256, dtype=torch.int32)\n",
    "    header[0] = 20240328  # Magic sayı\n",
    "    header[1] = 2  # Tokenizer versiyonu = 2 (1 -> 2: EOT token içerir)\n",
    "    header[2] = n  # Token sayısı\n",
    "    header[3] = enc.eot_token  # EOT (End-Of-Text) tokenı\n",
    "\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(header.numpy().tobytes())\n",
    "\n",
    "        # Her bir token için işlem yap\n",
    "        for i in range(n):\n",
    "            b = enc.decode_bytes([i])  # Tokenı baytlara çöz\n",
    "            length = len(b)\n",
    "            assert length < 256, f\"Token uzunluğu 255'i aşıyor: {length}\"\n",
    "            file.write(struct.pack(\"<B\", length))  # Uzunluğu 1 byte olarak yaz (unsigned integer)\n",
    "            file.write(b)  # Gerçek baytları yaz\n",
    "\n",
    "    print(f\"{filename} dosyası yazıldı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kullanılan cihaz: cpu (cpu)\n"
     ]
    }
   ],
   "source": [
    "# B, T = 2, 4  # Bu satır şu an yorum olarak işaretlenmiş durumda, kullanılmıyor gibi görünüyor.\n",
    "\n",
    "\"\"\" \n",
    "GPTConfig sınıfıyla ilgili açıklamalar burada yer alabilir. Bu sınıf, GPT modeli için yapılandırma parametrelerini içerir.\n",
    "\"\"\"\n",
    "\"\"\" @dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 128  # Blok boyutu\n",
    "    vocab_size: int = 64   # Kelime dağarcığı boyutu\n",
    "    n_layer: int = 4       # Katman sayısı\n",
    "    n_head: int = 4        # Kafa (head) sayısı\n",
    "    n_embd: int = 16       # Gömme boyutu\n",
    " \"\"\"\n",
    "B, T = 128, 32  # Batch boyutu ve zaman serisi uzunluğu tanımlanıyor.\n",
    "\n",
    "# Cihaz belirleme\n",
    "device = \"cpu\"  # Varsayılan olarak CPU kullanılıyor\n",
    "if torch.cuda.is_available():  # Eğer CUDA kullanılabilirse,\n",
    "    device = \"cuda\"  # CUDA cihazı kullanılıyor\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # CUDA Memory Pooling Subsystem (MPS) kullanılıyor\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # Cihaz tipi belirleme (cuda veya cpu)\n",
    "print(f\"Kullanılan cihaz: {device} ({device_type})\")  # Kullanılan cihazı ve tipini ekrana yazdırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp_rank = 0  # DDP (DistributedDataParallel) kümesindeki sürecin sırası (rank)\n",
    "ddp_local_rank = 0  # DDP kümesindeki yerel sıra (local rank)\n",
    "zero_stage = 0  # ZeRO-3 optimizasyonunun aşaması\n",
    "ddp_world_size = 1  # DDP kümesindeki toplam süreç sayısı (world size)\n",
    "master_process = True  # Ana süreç (master process) kontrolü\n",
    "seed_offset = 0  # Rastgelelik için başlangıç ofseti\n",
    "total_batch_size = B * T  # Toplam batch boyutu (Batch boyutu * Zaman serisi uzunluğu)\n",
    "tokens_per_fwdbwd = B * T  # İleri ve geri geçişte kullanılan toplam token sayısı\n",
    "tokens_per_fwdbwd  # Bu değişken aynı zamanda tokens_per_fwdbwd olarak tanımlanmıştır"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam istenen batch boyutu: 4096\n",
      "=> Hesaplanan gradient birikimi adımları: 1\n"
     ]
    }
   ],
   "source": [
    "# total_batch_size ve tokens_per_fwdbwd değişkenlerinin bölümünden kalanın sıfır olduğunu doğrulama\n",
    "assert total_batch_size % tokens_per_fwdbwd == 0\n",
    "\n",
    "# Gradient birikimi adımlarını hesaplamak için total_batch_size ve tokens_per_fwdbwd arasındaki bölümü gerçekleştiriyoruz\n",
    "grad_accum_steps = total_batch_size // tokens_per_fwdbwd\n",
    "\n",
    "# Hesaplanan gradient birikimi adımlarını ekrana yazdırma\n",
    "print(f\"Toplam istenen batch boyutu: {total_batch_size}\")\n",
    "print(f\"=> Hesaplanan gradient birikimi adımları: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext  # nullcontext'ı içe aktar\n",
    "\n",
    "# İstenilen dtype ve cihaz tipine göre bir bağlam yöneticisi kurma\n",
    "# 'float16' için ptdtype belirleme\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float8': torch.float8_e4m3fn}['float16']\n",
    "# Cihaz CUDA ise, otomatik karar verme (autocast) ile bağlam oluşturma\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "# Rastgelelik ve çoğaltılabilirlik (reproducibility)\n",
    "torch.manual_seed(42)  # CPU için rastgelelik tohumunu ayarlama\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)  # CUDA kullanılabilirse, CUDA için rastgelelik tohumunu ayarlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32, 144)\n",
       "    (wpe): Embedding(64, 144)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=144, out_features=432, bias=True)\n",
       "          (c_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=144, out_features=576, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=576, out_features=144, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=144, out_features=32, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yeni bir GPT modeli oluşturuyoruz ve yapılandırma nesnesi ile başlatıyoruz\n",
    "model = GPT(GPTConfig())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # Modeli eğitim moduna geçiriyoruz. Bu, modelin eğitim sırasında gradyanları takip etmesini sağlar.\n",
    "\n",
    "model.to(device)  # Modeli belirtilen cihaza taşıyoruz. 'device' değişkeni, önceden belirlenmiş bir cihaz türünü temsil eder (ör. 'cuda' veya 'cpu').\n",
    "\n",
    "if False:  # Koşul her zaman yanlış olduğu için bu blok çalıştırılmaz.\n",
    "    if hasattr(config, \"coordinate_descent_tuning\"):  # 'config' nesnesinin 'coordinate_descent_tuning' özelliği varsa,\n",
    "        config.coordinate_descent_tuning = True  # Bu özelliği 'True' olarak ayarlıyoruz. Bu ayar, modelin koordinat iniş ayarlamalarını etkinleştirmek için kullanılabilir.\n",
    "    print0(\"compiling the model...\")  # \"model derleniyor...\" mesajını ekrana yazdırıyoruz. 'print0' işlevi muhtemelen 'print' işlevini çağırmak için kullanılan bir işlevdir.\n",
    "    model = torch.compile(model)  # Modeli derlemek için 'torch.compile' işlevini kullanıyoruz. Ancak bu blok şu anda işletilmiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri Yükleyici: Toplam token sayısı: 1,520,922 1 dosyada\n",
      "Veri Yükleyici: Toplam token sayısı: 168,991 1 dosyada\n"
     ]
    }
   ],
   "source": [
    "# Eğitim için dağıtılmış veri yükleyicisi oluşturma\n",
    "train_loader = DistributedDataLoader(\"egitim_jetonlari.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "# \"egitim_jetonlari.bin\" dosyasından verileri yükler. B: batch boyutu, T: zaman serisi uzunluğu, ddp_rank: süreç sırası, ddp_world_size: toplam süreç sayısı.\n",
    "\n",
    "# Doğrulama için dağıtılmış veri yükleyicisi oluşturma\n",
    "val_loader = DistributedDataLoader(\"dogrulama_jetonlari.bin\", B, T, ddp_rank, ddp_world_size)\n",
    "# \"dogrulama_jetonlari.bin\" dosyasından verileri yükler. B: batch boyutu, T: zaman serisi uzunluğu, ddp_rank: süreç sırası, ddp_world_size: toplam süreç sayısı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 32]), torch.Size([128, 32]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "# 'train_loader' üzerinden bir sonraki batch'i alır. Bu işlev, dağıtılmış veri yükleyicisine özgü olabilir ve bir sonraki veri batch'ini döndürür.\n",
    "\n",
    "x, y = x.to(device), y.to(device)\n",
    "# 'x' ve 'y' tensörlerini belirtilen 'device' (cihaz) üzerine taşır. Bu işlem genellikle GPU'lar arasında veri transferi için kullanılır.\n",
    "\n",
    "x.shape, y.shape\n",
    "# 'x' ve 'y' tensörlerinin şekillerini yazdırır. Bu, her bir tensörün boyutlarını ve şeklini kontrol etmek için kullanışlıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 boyutundan 128 boyutlu doldurulmuş kelime dağarcığı\n",
      "gpt2_2.33M.bin dosyası yazıldı\n",
      "32 boyutundan 128 boyutlu doldurulmuş kelime dağarcığı\n",
      "gpt2_2.33M_bf16.bin dosyası yazıldı\n",
      "Referans gradyanlarda doldurulmuş kelime dağarcığı boyutu 32 boyutundan 128 boyutuna\n",
      "gpt2_2.33M_debug_state.bin dosyası yazıldı\n"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x, y)\n",
    "# Modeli, girdi ('x') ve etiket ('y') ile çağırarak çıktı tahminlerini ('logits') ve kaybı ('loss') hesaplar.\n",
    "\n",
    "loss.backward()\n",
    "# Kaybı kullanarak geri yayılım (backpropagation) işlemi yapar, yani gradyanları hesaplar ve modelin parametrelerine uygular.\n",
    "\n",
    "# Model parametrelerini farklı veri tiplerinde ('float32' ve 'bfloat16') kaydetme işlemi\n",
    "model_to_size = {\"tek-harfli-gpt\": \"2.33M\",  \"gpt2\": \"124M\", \"gpt2-medium\": \"355M\", \"gpt2-large\": \"774M\", \"gpt2-xl\": \"1558M\"}\n",
    "model_to_size.update({f\"d{d}\": f\"d{d}\" for d in [12, 24, 36, 48]})\n",
    "model_size_str = model_to_size[\"tek-harfli-gpt\"]  # Örneğin \"124M\" veya \"d12\"\n",
    "write_model(model, f\"gpt2_{model_size_str}.bin\", dtype=\"float16\")  # 'float16' veri tipinde modeli kaydeder\n",
    "write_model(model, f\"gpt2_{model_size_str}_bf16.bin\", dtype=\"bfloat16\")  # 'bfloat16' veri tipinde modeli kaydeder\n",
    "\n",
    "# Hata ayıklama için x, y, logits, loss ve gradyanları kaydetme işlemi\n",
    "# Referans olarak her zaman 'fp32' (float32) veri tipinde saklanır (?)\n",
    "write_state(model, x, y, logits, loss, f\"gpt2_{model_size_str}_debug_state.bin\")\n",
    "\n",
    "# Optimizasyon için train_loader'ı sıfırlama\n",
    "train_loader.reset()\n",
    "\n",
    "# Model gradyanlarını açıkça sıfırlama işlemi\n",
    "# Çünkü eğitim döngüsünde backward() ve ardından zero_grad() yaparız, bu da ilk eğitim adımında yanlış bir birikme yapabilir\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.0001\n",
    "# Ağırlıkların güncellenmesi sırasında uygulanacak L2 düzenlileştirmesi miktarı.\n",
    "\n",
    "learning_rate = 3e-5\n",
    "# Optimizasyon algoritması tarafından kullanılacak başlangıç öğrenme oranı.\n",
    "\n",
    "learning_rate_decay_frac = 0.0001\n",
    "# Her iterasyonda öğrenme oranının azaltılacağı yüzde miktarı.\n",
    "\n",
    "warmup_iters = 0\n",
    "# Eğitim başlamadan önce uygulanacak \"ısınma\" adımı sayısı.\n",
    "\n",
    "num_iterations = 1000\n",
    "# Eğitimin toplam iterasyon sayısı.\n",
    "\n",
    "val_loss_every = 0\n",
    "# Doğrulama kaybının hesaplanacağı iterasyon sayısı. Sıfır ise doğrulama her iterasyonda hesaplanmaz.\n",
    "\n",
    "val_max_steps = 20\n",
    "# Doğrulama işlemi için maksimum adım sayısı.\n",
    "\n",
    "sample_every = 0\n",
    "# Örnekleme sıklığı. Modelin her kaç iterasyonda bir örnek üreteceği.\n",
    "\n",
    "overfit_single_batch = 1\n",
    "# Tek bir batch üzerinde aşırı uyumu kontrol etmek için kullanılan faktör.\n",
    "\n",
    "inference_only = 0\n",
    "# Yalnızca çıkarım modunda çalıştırılıp çalıştırılmayacağı. 1 ise yalnızca çıkarım yapılır, eğitim yapılmaz.\n",
    "\n",
    "grad_clip = 1.0\n",
    "# Gradyanların maksimum normu. Bu değeri aşıldığında gradyanlar belirtilen değere göre kırpılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_model = model # Modeli 'ham_model' adlı bir değişkene kopyalama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decay edilen parametre tensörlerinin sayısı: 50, 2,999,808 parametre\n",
      "decay edilmeyen parametre tensörlerinin sayısı: 98, 22,752 parametre\n",
      "birleşik AdamW kullanılıyor: False\n",
      "normal AdamW kullanılıyor\n"
     ]
    }
   ],
   "source": [
    "optimizer = ham_model.configure_optimizers(\n",
    "    weight_decay=weight_decay,  # Ağırlıkların güncellenmesi sırasında uygulanacak L2 düzenlileştirmesi miktarı.\n",
    "    learning_rate=learning_rate,  # Optimizasyon algoritması tarafından kullanılacak öğrenme oranı.\n",
    "    betas=(0.9, 0.95),  # Adam optimizer için beta parametreleri. İlk beta momentum terimi için, ikinci beta RMSProp'un exponential decay terimi için kullanılır.\n",
    "    device_type=device,  # Kullanılan cihaz tipi (\"cuda\" veya \"cpu\").\n",
    "    zero_stage=zero_stage  # Zamanında adımlama (ZeRO) aşamasını belirtir. Bellek kullanımını azaltmak için büyük model eğitimlerinde kullanılır.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    min_lr = learning_rate * learning_rate_decay_frac\n",
    "    # 1) lineer warmup için warmup_iters adımı\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    # 2) eğer it > num_iterations ise min öğrenme oranını döndür\n",
    "    if it > num_iterations:\n",
    "        return min_lr\n",
    "    # 3) ara değerlerde, min öğrenme oranına kadar kosinüs azalma kullan\n",
    "    decay_ratio = (it - warmup_iters) / (num_iterations - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff 1'den başlar ve 0'a kadar iner\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = ['a', 'b', 'c', 'ç', 'd', 'e', 'f', 'g', 'ğ', 'h', 'ı', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'ö', 'p', 'r', 's', 'ş', 't', 'u', 'ü', 'v', 'y', 'z', \n",
    "               ' ', '.', ',']\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Verilen metni harf listesindeki indislerle kodlar.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Kodlanacak metin.\n",
    "\n",
    "    Returns:\n",
    "    - list: Metnin harf listesindeki indislerle kodlanmış hali.\n",
    "    \"\"\"\n",
    "    return [letter_list.index(c) for c in text.lower()]\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"\n",
    "    Verilen indis listesini orijinal metne dönüştürür.\n",
    "\n",
    "    Args:\n",
    "    - ids (list): Kodlanmış metnin indis listesi.\n",
    "\n",
    "    Returns:\n",
    "    - str: Kodlanmış indislerden orijinal metni oluşturan metin.\n",
    "    \"\"\"\n",
    "    return ''.join([letter_list[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0385,  0.0297,  0.0180, -0.0421,  0.0136],\n",
       "        [-0.0195,  0.0192,  0.0324,  0.0290,  0.0054],\n",
       "        [ 0.0195, -0.0203, -0.0108, -0.0088, -0.0063],\n",
       "        [-0.0099,  0.0227, -0.0092,  0.0284,  0.0170],\n",
       "        [-0.0161, -0.0224,  0.0039, -0.0156, -0.0358]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Örnek giriş:  \n",
      "Üretilen çıktı:  zugebblujlypdğoğutyeddddlhtbeşüp\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "def calistir():\n",
    "    # Örnek bir metin oluşturmak için bir örnek\n",
    "    sample_text = \" \"\n",
    "\n",
    "    # Metni harf listesi indisleriyle kodlayıp tensor olarak dönüştürme\n",
    "    sample_tokens = encode(sample_text)\n",
    "    sample_tokens = torch.tensor(sample_tokens, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "    # Modelin değerlendirme moduna geçmesi\n",
    "    model.eval()\n",
    "\n",
    "    # Gradyan hesaplama olmadan metin üretme işlemi yapma\n",
    "    with torch.no_grad():\n",
    "        sample_out = model.generate(sample_tokens, max_new_tokens=32, temperature=0.95, top_k=10)\n",
    "\n",
    "    # Üretilen metni ekrana yazdırma\n",
    "    print('---------------')\n",
    "    print(f\"Örnek giriş: {sample_text}\")\n",
    "    print(f\"Üretilen çıktı: {decode(sample_out[0].tolist())}\")\n",
    "    print('---------------')\n",
    "\n",
    "# Fonksiyonu çalıştırma\n",
    "calistir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1/1000 | train loss 3.540723 | norm 5.9734 | lr 3.00e-05 | (584.60 ms | 7007 tok/s)\n",
      "step    2/1000 | train loss 3.419310 | norm 4.5247 | lr 3.00e-05 | (515.09 ms | 7952 tok/s)\n",
      "step    3/1000 | train loss 3.338907 | norm 3.0462 | lr 3.00e-05 | (487.84 ms | 8396 tok/s)\n",
      "step    4/1000 | train loss 3.287021 | norm 2.4048 | lr 3.00e-05 | (503.01 ms | 8143 tok/s)\n",
      "step    5/1000 | train loss 3.248993 | norm 2.0831 | lr 3.00e-05 | (484.90 ms | 8447 tok/s)\n",
      "step    6/1000 | train loss 3.218313 | norm 1.8324 | lr 3.00e-05 | (468.06 ms | 8751 tok/s)\n",
      "step    7/1000 | train loss 3.192699 | norm 1.6176 | lr 3.00e-05 | (481.59 ms | 8505 tok/s)\n",
      "step    8/1000 | train loss 3.171211 | norm 1.4465 | lr 3.00e-05 | (585.85 ms | 6992 tok/s)\n",
      "step    9/1000 | train loss 3.153162 | norm 1.3238 | lr 3.00e-05 | (533.82 ms | 7673 tok/s)\n",
      "step   10/1000 | train loss 3.137821 | norm 1.2433 | lr 3.00e-05 | (544.64 ms | 7521 tok/s)\n",
      "step   11/1000 | train loss 3.124435 | norm 1.1920 | lr 3.00e-05 | (532.17 ms | 7697 tok/s)\n",
      "step   12/1000 | train loss 3.112347 | norm 1.1576 | lr 3.00e-05 | (508.56 ms | 8054 tok/s)\n",
      "step   13/1000 | train loss 3.101062 | norm 1.1332 | lr 3.00e-05 | (511.81 ms | 8003 tok/s)\n",
      "step   14/1000 | train loss 3.090228 | norm 1.1171 | lr 3.00e-05 | (486.00 ms | 8428 tok/s)\n",
      "step   15/1000 | train loss 3.079571 | norm 1.1109 | lr 3.00e-05 | (550.17 ms | 7445 tok/s)\n",
      "step   16/1000 | train loss 3.068832 | norm 1.1165 | lr 3.00e-05 | (538.50 ms | 7606 tok/s)\n",
      "step   17/1000 | train loss 3.057742 | norm 1.1345 | lr 3.00e-05 | (571.36 ms | 7169 tok/s)\n",
      "step   18/1000 | train loss 3.046043 | norm 1.1649 | lr 3.00e-05 | (601.09 ms | 6814 tok/s)\n",
      "step   19/1000 | train loss 3.033543 | norm 1.2055 | lr 3.00e-05 | (576.69 ms | 7103 tok/s)\n",
      "step   20/1000 | train loss 3.020164 | norm 1.2500 | lr 3.00e-05 | (545.64 ms | 7507 tok/s)\n",
      "step   21/1000 | train loss 3.006034 | norm 1.2921 | lr 3.00e-05 | (596.94 ms | 6862 tok/s)\n",
      "step   22/1000 | train loss 2.991480 | norm 1.3430 | lr 3.00e-05 | (503.58 ms | 8134 tok/s)\n",
      "step   23/1000 | train loss 2.976743 | norm 1.4055 | lr 3.00e-05 | (482.97 ms | 8481 tok/s)\n",
      "step   24/1000 | train loss 2.961831 | norm 1.4205 | lr 3.00e-05 | (484.78 ms | 8449 tok/s)\n",
      "step   25/1000 | train loss 2.946952 | norm 1.3825 | lr 3.00e-05 | (498.49 ms | 8217 tok/s)\n",
      "step   26/1000 | train loss 2.932455 | norm 1.3633 | lr 3.00e-05 | (529.64 ms | 7734 tok/s)\n",
      "step   27/1000 | train loss 2.918368 | norm 1.3597 | lr 2.99e-05 | (490.51 ms | 8351 tok/s)\n",
      "step   28/1000 | train loss 2.904645 | norm 1.3423 | lr 2.99e-05 | (577.81 ms | 7089 tok/s)\n",
      "step   29/1000 | train loss 2.891382 | norm 1.3475 | lr 2.99e-05 | (542.61 ms | 7549 tok/s)\n",
      "step   30/1000 | train loss 2.878456 | norm 1.3660 | lr 2.99e-05 | (609.24 ms | 6723 tok/s)\n",
      "step   31/1000 | train loss 2.865668 | norm 1.3449 | lr 2.99e-05 | (557.27 ms | 7350 tok/s)\n",
      "step   32/1000 | train loss 2.853131 | norm 1.3223 | lr 2.99e-05 | (573.03 ms | 7148 tok/s)\n",
      "step   33/1000 | train loss 2.840925 | norm 1.3235 | lr 2.99e-05 | (546.98 ms | 7488 tok/s)\n",
      "step   34/1000 | train loss 2.828932 | norm 1.3158 | lr 2.99e-05 | (573.86 ms | 7138 tok/s)\n",
      "step   35/1000 | train loss 2.817124 | norm 1.3020 | lr 2.99e-05 | (504.68 ms | 8116 tok/s)\n",
      "step   36/1000 | train loss 2.805555 | norm 1.2890 | lr 2.99e-05 | (479.17 ms | 8548 tok/s)\n",
      "step   37/1000 | train loss 2.794240 | norm 1.2754 | lr 2.99e-05 | (447.02 ms | 9163 tok/s)\n",
      "step   38/1000 | train loss 2.783190 | norm 1.2587 | lr 2.99e-05 | (497.22 ms | 8238 tok/s)\n",
      "step   39/1000 | train loss 2.772481 | norm 1.2506 | lr 2.99e-05 | (513.10 ms | 7983 tok/s)\n",
      "step   40/1000 | train loss 2.762150 | norm 1.2438 | lr 2.99e-05 | (579.46 ms | 7069 tok/s)\n",
      "step   41/1000 | train loss 2.752209 | norm 1.2210 | lr 2.99e-05 | (536.59 ms | 7633 tok/s)\n",
      "step   42/1000 | train loss 2.742707 | norm 1.2192 | lr 2.99e-05 | (593.22 ms | 6905 tok/s)\n",
      "step   43/1000 | train loss 2.733555 | norm 1.2030 | lr 2.99e-05 | (686.30 ms | 5968 tok/s)\n",
      "step   44/1000 | train loss 2.724767 | norm 1.1995 | lr 2.99e-05 | (572.01 ms | 7161 tok/s)\n",
      "step   45/1000 | train loss 2.716266 | norm 1.1903 | lr 2.99e-05 | (525.97 ms | 7787 tok/s)\n",
      "step   46/1000 | train loss 2.707992 | norm 1.1827 | lr 2.99e-05 | (536.70 ms | 7632 tok/s)\n",
      "step   47/1000 | train loss 2.699917 | norm 1.1768 | lr 2.98e-05 | (592.07 ms | 6918 tok/s)\n",
      "step   48/1000 | train loss 2.691995 | norm 1.1676 | lr 2.98e-05 | (595.57 ms | 6877 tok/s)\n",
      "step   49/1000 | train loss 2.684219 | norm 1.1665 | lr 2.98e-05 | (460.74 ms | 8890 tok/s)\n",
      "step   50/1000 | train loss 2.676535 | norm 1.1550 | lr 2.98e-05 | (478.35 ms | 8563 tok/s)\n",
      "step   51/1000 | train loss 2.668965 | norm 1.1499 | lr 2.98e-05 | (482.35 ms | 8492 tok/s)\n",
      "step   52/1000 | train loss 2.661477 | norm 1.1427 | lr 2.98e-05 | (487.14 ms | 8408 tok/s)\n",
      "step   53/1000 | train loss 2.654070 | norm 1.1474 | lr 2.98e-05 | (563.90 ms | 7264 tok/s)\n",
      "step   54/1000 | train loss 2.646714 | norm 1.1364 | lr 2.98e-05 | (602.18 ms | 6802 tok/s)\n",
      "step   55/1000 | train loss 2.639411 | norm 1.1332 | lr 2.98e-05 | (590.51 ms | 6936 tok/s)\n",
      "step   56/1000 | train loss 2.632155 | norm 1.1272 | lr 2.98e-05 | (558.65 ms | 7332 tok/s)\n",
      "step   57/1000 | train loss 2.624965 | norm 1.1257 | lr 2.98e-05 | (557.02 ms | 7353 tok/s)\n",
      "step   58/1000 | train loss 2.617828 | norm 1.1287 | lr 2.98e-05 | (492.81 ms | 8311 tok/s)\n",
      "step   59/1000 | train loss 2.610711 | norm 1.1489 | lr 2.98e-05 | (527.74 ms | 7761 tok/s)\n",
      "step   60/1000 | train loss 2.603671 | norm 1.2734 | lr 2.97e-05 | (478.20 ms | 8566 tok/s)\n",
      "step   61/1000 | train loss 2.596919 | norm 1.6246 | lr 2.97e-05 | (482.59 ms | 8487 tok/s)\n",
      "step   62/1000 | train loss 2.589994 | norm 1.4648 | lr 2.97e-05 | (459.68 ms | 8910 tok/s)\n",
      "step   63/1000 | train loss 2.583139 | norm 1.1840 | lr 2.97e-05 | (576.54 ms | 7104 tok/s)\n",
      "step   64/1000 | train loss 2.576345 | norm 1.1254 | lr 2.97e-05 | (591.98 ms | 6919 tok/s)\n",
      "step   65/1000 | train loss 2.569497 | norm 1.2342 | lr 2.97e-05 | (511.46 ms | 8008 tok/s)\n",
      "step   66/1000 | train loss 2.562830 | norm 1.6692 | lr 2.97e-05 | (473.42 ms | 8652 tok/s)\n",
      "step   67/1000 | train loss 2.556064 | norm 1.7198 | lr 2.97e-05 | (525.77 ms | 7791 tok/s)\n",
      "step   68/1000 | train loss 2.549161 | norm 1.2839 | lr 2.97e-05 | (635.14 ms | 6449 tok/s)\n",
      "step   69/1000 | train loss 2.542338 | norm 1.1555 | lr 2.97e-05 | (623.42 ms | 6570 tok/s)\n",
      "step   70/1000 | train loss 2.535397 | norm 1.1822 | lr 2.96e-05 | (547.40 ms | 7483 tok/s)\n",
      "step   71/1000 | train loss 2.528442 | norm 1.5393 | lr 2.96e-05 | (466.72 ms | 8776 tok/s)\n",
      "step   72/1000 | train loss 2.521935 | norm 2.4161 | lr 2.96e-05 | (495.22 ms | 8271 tok/s)\n",
      "step   73/1000 | train loss 2.514517 | norm 1.3757 | lr 2.96e-05 | (492.79 ms | 8312 tok/s)\n",
      "step   74/1000 | train loss 2.507539 | norm 1.1694 | lr 2.96e-05 | (478.89 ms | 8553 tok/s)\n",
      "step   75/1000 | train loss 2.500417 | norm 1.2929 | lr 2.96e-05 | (505.12 ms | 8109 tok/s)\n",
      "step   76/1000 | train loss 2.493737 | norm 2.5483 | lr 2.96e-05 | (564.23 ms | 7259 tok/s)\n",
      "step   77/1000 | train loss 2.486568 | norm 2.5795 | lr 2.96e-05 | (528.40 ms | 7752 tok/s)\n",
      "step   78/1000 | train loss 2.479233 | norm 1.5515 | lr 2.96e-05 | (535.06 ms | 7655 tok/s)\n",
      "step   79/1000 | train loss 2.472434 | norm 2.0795 | lr 2.96e-05 | (522.70 ms | 7836 tok/s)\n",
      "step   80/1000 | train loss 2.465840 | norm 2.8291 | lr 2.95e-05 | (555.16 ms | 7378 tok/s)\n",
      "step   81/1000 | train loss 2.458440 | norm 1.8310 | lr 2.95e-05 | (567.55 ms | 7217 tok/s)\n",
      "step   82/1000 | train loss 2.452649 | norm 3.4658 | lr 2.95e-05 | (519.29 ms | 7888 tok/s)\n",
      "step   83/1000 | train loss 2.444835 | norm 1.9251 | lr 2.95e-05 | (456.00 ms | 8983 tok/s)\n",
      "step   84/1000 | train loss 2.438186 | norm 2.6100 | lr 2.95e-05 | (481.53 ms | 8506 tok/s)\n",
      "step   85/1000 | train loss 2.431967 | norm 3.9563 | lr 2.95e-05 | (492.42 ms | 8318 tok/s)\n",
      "step   86/1000 | train loss 2.424424 | norm 1.5673 | lr 2.95e-05 | (516.64 ms | 7928 tok/s)\n",
      "step   87/1000 | train loss 2.418337 | norm 3.3140 | lr 2.95e-05 | (583.59 ms | 7019 tok/s)\n",
      "step   88/1000 | train loss 2.411632 | norm 3.7258 | lr 2.94e-05 | (521.56 ms | 7853 tok/s)\n",
      "step   89/1000 | train loss 2.404426 | norm 2.8685 | lr 2.94e-05 | (517.16 ms | 7920 tok/s)\n",
      "step   90/1000 | train loss 2.398865 | norm 4.2192 | lr 2.94e-05 | (586.43 ms | 6985 tok/s)\n",
      "step   91/1000 | train loss 2.390899 | norm 1.2225 | lr 2.94e-05 | (578.75 ms | 7077 tok/s)\n",
      "step   92/1000 | train loss 2.385732 | norm 4.5605 | lr 2.94e-05 | (612.14 ms | 6691 tok/s)\n",
      "step   93/1000 | train loss 2.376947 | norm 1.5205 | lr 2.94e-05 | (571.28 ms | 7170 tok/s)\n",
      "step   94/1000 | train loss 2.370536 | norm 4.1665 | lr 2.94e-05 | (462.17 ms | 8863 tok/s)\n",
      "step   95/1000 | train loss 2.364410 | norm 6.8693 | lr 2.94e-05 | (555.52 ms | 7373 tok/s)\n",
      "step   96/1000 | train loss 2.356006 | norm 2.4013 | lr 2.93e-05 | (502.68 ms | 8148 tok/s)\n",
      "step   97/1000 | train loss 2.350480 | norm 4.7654 | lr 2.93e-05 | (521.83 ms | 7849 tok/s)\n",
      "step   98/1000 | train loss 2.342430 | norm 2.4992 | lr 2.93e-05 | (610.57 ms | 6708 tok/s)\n",
      "step   99/1000 | train loss 2.338368 | norm 8.4420 | lr 2.93e-05 | (558.16 ms | 7338 tok/s)\n",
      "step  100/1000 | train loss 2.331906 | norm 5.3982 | lr 2.93e-05 | (450.53 ms | 9092 tok/s)\n",
      "step  101/1000 | train loss 2.322696 | norm 2.7116 | lr 2.93e-05 | (490.90 ms | 8344 tok/s)\n",
      "step  102/1000 | train loss 2.320469 | norm 8.6001 | lr 2.93e-05 | (572.36 ms | 7156 tok/s)\n",
      "step  103/1000 | train loss 2.313023 | norm 6.8829 | lr 2.92e-05 | (560.82 ms | 7304 tok/s)\n",
      "step  104/1000 | train loss 2.305311 | norm 4.9736 | lr 2.92e-05 | (552.36 ms | 7415 tok/s)\n",
      "step  105/1000 | train loss 2.298968 | norm 3.2014 | lr 2.92e-05 | (523.41 ms | 7826 tok/s)\n",
      "step  106/1000 | train loss 2.294471 | norm 6.4294 | lr 2.92e-05 | (499.99 ms | 8192 tok/s)\n",
      "step  107/1000 | train loss 2.287987 | norm 5.9194 | lr 2.92e-05 | (502.59 ms | 8150 tok/s)\n",
      "step  108/1000 | train loss 2.284153 | norm 10.3046 | lr 2.92e-05 | (518.39 ms | 7901 tok/s)\n",
      "step  109/1000 | train loss 2.277210 | norm 6.4642 | lr 2.91e-05 | (534.80 ms | 7659 tok/s)\n",
      "step  110/1000 | train loss 2.270928 | norm 6.1759 | lr 2.91e-05 | (566.77 ms | 7227 tok/s)\n",
      "step  111/1000 | train loss 2.265416 | norm 4.5574 | lr 2.91e-05 | (477.02 ms | 8587 tok/s)\n",
      "step  112/1000 | train loss 2.259343 | norm 4.8541 | lr 2.91e-05 | (561.82 ms | 7291 tok/s)\n",
      "step  113/1000 | train loss 2.255862 | norm 7.2240 | lr 2.91e-05 | (532.32 ms | 7695 tok/s)\n",
      "step  114/1000 | train loss 2.249408 | norm 9.6795 | lr 2.91e-05 | (550.03 ms | 7447 tok/s)\n",
      "step  115/1000 | train loss 2.244686 | norm 10.2344 | lr 2.90e-05 | (574.45 ms | 7130 tok/s)\n",
      "step  116/1000 | train loss 2.239704 | norm 7.6367 | lr 2.90e-05 | (561.10 ms | 7300 tok/s)\n",
      "step  117/1000 | train loss 2.232574 | norm 5.2078 | lr 2.90e-05 | (442.85 ms | 9249 tok/s)\n",
      "step  118/1000 | train loss 2.229442 | norm 10.3709 | lr 2.90e-05 | (520.10 ms | 7875 tok/s)\n",
      "step  119/1000 | train loss 2.226448 | norm 13.1971 | lr 2.90e-05 | (563.83 ms | 7265 tok/s)\n",
      "step  120/1000 | train loss 2.216840 | norm 3.3135 | lr 2.90e-05 | (493.14 ms | 8306 tok/s)\n",
      "step  121/1000 | train loss 2.224518 | norm 22.1679 | lr 2.89e-05 | (484.41 ms | 8456 tok/s)\n",
      "step  122/1000 | train loss 2.223946 | norm 18.2082 | lr 2.89e-05 | (592.70 ms | 6911 tok/s)\n",
      "step  123/1000 | train loss 2.212781 | norm 17.0209 | lr 2.89e-05 | (557.35 ms | 7349 tok/s)\n",
      "step  124/1000 | train loss 2.202022 | norm 10.1902 | lr 2.89e-05 | (502.56 ms | 8150 tok/s)\n",
      "step  125/1000 | train loss 2.198359 | norm 13.8295 | lr 2.89e-05 | (487.37 ms | 8404 tok/s)\n",
      "step  126/1000 | train loss 2.198665 | norm 14.8578 | lr 2.89e-05 | (581.19 ms | 7048 tok/s)\n",
      "step  127/1000 | train loss 2.191274 | norm 10.5389 | lr 2.88e-05 | (592.90 ms | 6908 tok/s)\n",
      "step  128/1000 | train loss 2.186103 | norm 17.2725 | lr 2.88e-05 | (586.79 ms | 6980 tok/s)\n",
      "step  129/1000 | train loss 2.185061 | norm 13.0716 | lr 2.88e-05 | (584.93 ms | 7003 tok/s)\n",
      "step  130/1000 | train loss 2.180214 | norm 13.9613 | lr 2.88e-05 | (569.47 ms | 7193 tok/s)\n",
      "step  131/1000 | train loss 2.176125 | norm 18.0556 | lr 2.88e-05 | (507.18 ms | 8076 tok/s)\n",
      "step  132/1000 | train loss 2.169581 | norm 13.6475 | lr 2.87e-05 | (475.59 ms | 8612 tok/s)\n",
      "step  133/1000 | train loss 2.169625 | norm 20.2469 | lr 2.87e-05 | (482.45 ms | 8490 tok/s)\n",
      "step  134/1000 | train loss 2.167913 | norm 16.0005 | lr 2.87e-05 | (471.59 ms | 8686 tok/s)\n",
      "step  135/1000 | train loss 2.164003 | norm 20.8708 | lr 2.87e-05 | (546.02 ms | 7502 tok/s)\n",
      "step  136/1000 | train loss 2.151154 | norm 6.9495 | lr 2.87e-05 | (532.17 ms | 7697 tok/s)\n",
      "step  137/1000 | train loss 2.167373 | norm 31.4650 | lr 2.87e-05 | (463.67 ms | 8834 tok/s)\n",
      "step  138/1000 | train loss 2.161869 | norm 26.8227 | lr 2.86e-05 | (468.52 ms | 8742 tok/s)\n",
      "step  139/1000 | train loss 2.150087 | norm 18.6163 | lr 2.86e-05 | (548.09 ms | 7473 tok/s)\n",
      "step  140/1000 | train loss 2.144579 | norm 21.5556 | lr 2.86e-05 | (634.47 ms | 6456 tok/s)\n",
      "step  141/1000 | train loss 2.138764 | norm 15.2471 | lr 2.86e-05 | (535.69 ms | 7646 tok/s)\n",
      "step  142/1000 | train loss 2.133922 | norm 11.4607 | lr 2.86e-05 | (562.81 ms | 7278 tok/s)\n",
      "step  143/1000 | train loss 2.139925 | norm 26.1288 | lr 2.85e-05 | (580.68 ms | 7054 tok/s)\n",
      "step  144/1000 | train loss 2.132457 | norm 20.5556 | lr 2.85e-05 | (562.76 ms | 7278 tok/s)\n",
      "step  145/1000 | train loss 2.128528 | norm 20.8482 | lr 2.85e-05 | (489.50 ms | 8368 tok/s)\n",
      "step  146/1000 | train loss 2.124592 | norm 18.8914 | lr 2.85e-05 | (494.65 ms | 8281 tok/s)\n",
      "step  147/1000 | train loss 2.124111 | norm 22.4359 | lr 2.84e-05 | (486.91 ms | 8412 tok/s)\n",
      "step  148/1000 | train loss 2.118025 | norm 20.7599 | lr 2.84e-05 | (473.96 ms | 8642 tok/s)\n",
      "step  149/1000 | train loss 2.112139 | norm 16.2406 | lr 2.84e-05 | (462.09 ms | 8864 tok/s)\n",
      "step  150/1000 | train loss 2.109127 | norm 14.4728 | lr 2.84e-05 | (448.71 ms | 9128 tok/s)\n",
      "step  151/1000 | train loss 2.108058 | norm 20.9238 | lr 2.84e-05 | (556.79 ms | 7356 tok/s)\n",
      "step  152/1000 | train loss 2.103323 | norm 16.1132 | lr 2.83e-05 | (599.66 ms | 6831 tok/s)\n",
      "step  153/1000 | train loss 2.102740 | norm 22.0793 | lr 2.83e-05 | (542.84 ms | 7546 tok/s)\n",
      "step  154/1000 | train loss 2.098960 | norm 19.5222 | lr 2.83e-05 | (585.76 ms | 6993 tok/s)\n",
      "step  155/1000 | train loss 2.094427 | norm 18.6803 | lr 2.83e-05 | (542.33 ms | 7553 tok/s)\n",
      "step  156/1000 | train loss 2.089164 | norm 17.5856 | lr 2.83e-05 | (582.59 ms | 7031 tok/s)\n",
      "step  157/1000 | train loss 2.087488 | norm 18.1418 | lr 2.82e-05 | (505.32 ms | 8106 tok/s)\n",
      "step  158/1000 | train loss 2.081736 | norm 15.0463 | lr 2.82e-05 | (501.46 ms | 8168 tok/s)\n",
      "step  159/1000 | train loss 2.081394 | norm 20.6637 | lr 2.82e-05 | (467.14 ms | 8768 tok/s)\n",
      "step  160/1000 | train loss 2.075460 | norm 16.6430 | lr 2.82e-05 | (524.34 ms | 7812 tok/s)\n",
      "step  161/1000 | train loss 2.074406 | norm 20.2339 | lr 2.81e-05 | (503.22 ms | 8140 tok/s)\n",
      "step  162/1000 | train loss 2.068602 | norm 16.6885 | lr 2.81e-05 | (545.66 ms | 7506 tok/s)\n",
      "step  163/1000 | train loss 2.067680 | norm 21.3999 | lr 2.81e-05 | (551.35 ms | 7429 tok/s)\n",
      "step  164/1000 | train loss 2.060762 | norm 17.6548 | lr 2.81e-05 | (569.98 ms | 7186 tok/s)\n",
      "step  165/1000 | train loss 2.061244 | norm 20.8982 | lr 2.81e-05 | (600.34 ms | 6823 tok/s)\n",
      "step  166/1000 | train loss 2.054076 | norm 19.0445 | lr 2.80e-05 | (589.27 ms | 6951 tok/s)\n",
      "step  167/1000 | train loss 2.050859 | norm 17.8858 | lr 2.80e-05 | (572.63 ms | 7153 tok/s)\n",
      "step  168/1000 | train loss 2.044984 | norm 16.4677 | lr 2.80e-05 | (564.43 ms | 7257 tok/s)\n",
      "step  169/1000 | train loss 2.042906 | norm 17.8678 | lr 2.80e-05 | (473.90 ms | 8643 tok/s)\n",
      "step  170/1000 | train loss 2.037270 | norm 15.1674 | lr 2.79e-05 | (466.29 ms | 8784 tok/s)\n",
      "step  171/1000 | train loss 2.035180 | norm 19.5053 | lr 2.79e-05 | (466.77 ms | 8775 tok/s)\n",
      "step  172/1000 | train loss 2.031060 | norm 17.0784 | lr 2.79e-05 | (460.26 ms | 8899 tok/s)\n",
      "step  173/1000 | train loss 2.024891 | norm 17.1753 | lr 2.79e-05 | (568.49 ms | 7205 tok/s)\n",
      "step  174/1000 | train loss 2.022179 | norm 14.8365 | lr 2.78e-05 | (612.00 ms | 6693 tok/s)\n",
      "step  175/1000 | train loss 2.015998 | norm 16.7691 | lr 2.78e-05 | (562.45 ms | 7282 tok/s)\n",
      "step  176/1000 | train loss 2.012834 | norm 13.8750 | lr 2.78e-05 | (542.22 ms | 7554 tok/s)\n",
      "step  177/1000 | train loss 2.006614 | norm 14.1537 | lr 2.78e-05 | (637.18 ms | 6428 tok/s)\n",
      "step  178/1000 | train loss 2.003316 | norm 13.6647 | lr 2.77e-05 | (565.49 ms | 7243 tok/s)\n",
      "step  179/1000 | train loss 1.995778 | norm 10.1946 | lr 2.77e-05 | (574.07 ms | 7135 tok/s)\n",
      "step  180/1000 | train loss 1.998660 | norm 17.4062 | lr 2.77e-05 | (485.96 ms | 8429 tok/s)\n",
      "step  181/1000 | train loss 1.987707 | norm 11.5367 | lr 2.77e-05 | (513.07 ms | 7983 tok/s)\n",
      "step  182/1000 | train loss 1.992820 | norm 21.8243 | lr 2.76e-05 | (500.80 ms | 8179 tok/s)\n",
      "step  183/1000 | train loss 1.991185 | norm 22.4584 | lr 2.76e-05 | (517.65 ms | 7913 tok/s)\n",
      "step  184/1000 | train loss 1.971212 | norm 9.7594 | lr 2.76e-05 | (535.48 ms | 7649 tok/s)\n",
      "step  185/1000 | train loss 1.979385 | norm 21.7738 | lr 2.76e-05 | (642.37 ms | 6376 tok/s)\n",
      "step  186/1000 | train loss 1.976072 | norm 20.4318 | lr 2.75e-05 | (641.33 ms | 6387 tok/s)\n",
      "step  187/1000 | train loss 1.956439 | norm 9.0869 | lr 2.75e-05 | (539.80 ms | 7588 tok/s)\n",
      "step  188/1000 | train loss 1.983840 | norm 37.9071 | lr 2.75e-05 | (566.10 ms | 7235 tok/s)\n",
      "step  189/1000 | train loss 1.988254 | norm 34.8525 | lr 2.75e-05 | (724.07 ms | 5657 tok/s)\n",
      "step  190/1000 | train loss 1.965723 | norm 27.2656 | lr 2.74e-05 | (604.97 ms | 6771 tok/s)\n",
      "step  191/1000 | train loss 1.947214 | norm 22.8595 | lr 2.74e-05 | (583.12 ms | 7024 tok/s)\n",
      "step  192/1000 | train loss 1.952836 | norm 28.2276 | lr 2.74e-05 | (475.81 ms | 8609 tok/s)\n",
      "step  193/1000 | train loss 1.957650 | norm 28.6840 | lr 2.74e-05 | (598.02 ms | 6849 tok/s)\n",
      "step  194/1000 | train loss 1.940850 | norm 23.4067 | lr 2.73e-05 | (560.63 ms | 7306 tok/s)\n",
      "step  195/1000 | train loss 1.930100 | norm 21.4986 | lr 2.73e-05 | (468.48 ms | 8743 tok/s)\n",
      "step  196/1000 | train loss 1.931124 | norm 18.7721 | lr 2.73e-05 | (483.77 ms | 8467 tok/s)\n",
      "step  197/1000 | train loss 1.928123 | norm 21.8905 | lr 2.72e-05 | (532.08 ms | 7698 tok/s)\n",
      "step  198/1000 | train loss 1.913976 | norm 11.7811 | lr 2.72e-05 | (585.21 ms | 6999 tok/s)\n",
      "step  199/1000 | train loss 1.911521 | norm 11.7182 | lr 2.72e-05 | (534.64 ms | 7661 tok/s)\n",
      "step  200/1000 | train loss 1.911123 | norm 22.6817 | lr 2.72e-05 | (584.77 ms | 7004 tok/s)\n",
      "step  201/1000 | train loss 1.900610 | norm 10.4488 | lr 2.71e-05 | (598.60 ms | 6843 tok/s)\n",
      "step  202/1000 | train loss 1.904643 | norm 24.6629 | lr 2.71e-05 | (503.34 ms | 8138 tok/s)\n",
      "step  203/1000 | train loss 1.899018 | norm 14.9269 | lr 2.71e-05 | (499.95 ms | 8193 tok/s)\n",
      "step  204/1000 | train loss 1.894770 | norm 23.7562 | lr 2.71e-05 | (542.73 ms | 7547 tok/s)\n",
      "step  205/1000 | train loss 1.885721 | norm 15.5200 | lr 2.70e-05 | (536.03 ms | 7641 tok/s)\n",
      "step  206/1000 | train loss 1.892029 | norm 28.8192 | lr 2.70e-05 | (559.69 ms | 7318 tok/s)\n",
      "step  207/1000 | train loss 1.882996 | norm 22.9553 | lr 2.70e-05 | (494.54 ms | 8282 tok/s)\n",
      "step  208/1000 | train loss 1.875689 | norm 22.5628 | lr 2.69e-05 | (571.30 ms | 7170 tok/s)\n",
      "step  209/1000 | train loss 1.873843 | norm 22.8516 | lr 2.69e-05 | (567.56 ms | 7217 tok/s)\n",
      "step  210/1000 | train loss 1.865505 | norm 17.0140 | lr 2.69e-05 | (559.94 ms | 7315 tok/s)\n",
      "step  211/1000 | train loss 1.862032 | norm 19.6343 | lr 2.69e-05 | (570.86 ms | 7175 tok/s)\n",
      "step  212/1000 | train loss 1.855383 | norm 15.5091 | lr 2.68e-05 | (504.54 ms | 8118 tok/s)\n",
      "step  213/1000 | train loss 1.856397 | norm 24.2014 | lr 2.68e-05 | (493.08 ms | 8307 tok/s)\n",
      "step  214/1000 | train loss 1.845608 | norm 13.8271 | lr 2.68e-05 | (514.10 ms | 7967 tok/s)\n",
      "step  215/1000 | train loss 1.852646 | norm 29.2850 | lr 2.67e-05 | (480.10 ms | 8532 tok/s)\n",
      "step  216/1000 | train loss 1.847027 | norm 25.4257 | lr 2.67e-05 | (539.86 ms | 7587 tok/s)\n",
      "step  217/1000 | train loss 1.836834 | norm 18.4554 | lr 2.67e-05 | (558.89 ms | 7329 tok/s)\n",
      "step  218/1000 | train loss 1.831207 | norm 18.5261 | lr 2.66e-05 | (594.73 ms | 6887 tok/s)\n",
      "step  219/1000 | train loss 1.827394 | norm 18.1844 | lr 2.66e-05 | (634.47 ms | 6456 tok/s)\n",
      "step  220/1000 | train loss 1.825336 | norm 21.7522 | lr 2.66e-05 | (591.77 ms | 6922 tok/s)\n",
      "step  221/1000 | train loss 1.815442 | norm 15.7309 | lr 2.66e-05 | (576.20 ms | 7109 tok/s)\n",
      "step  222/1000 | train loss 1.818607 | norm 26.9200 | lr 2.65e-05 | (568.28 ms | 7208 tok/s)\n",
      "step  223/1000 | train loss 1.811953 | norm 18.4478 | lr 2.65e-05 | (549.89 ms | 7449 tok/s)\n",
      "step  224/1000 | train loss 1.805907 | norm 20.8856 | lr 2.65e-05 | (557.23 ms | 7351 tok/s)\n",
      "step  225/1000 | train loss 1.801654 | norm 17.7244 | lr 2.64e-05 | (514.82 ms | 7956 tok/s)\n",
      "step  226/1000 | train loss 1.800440 | norm 29.2583 | lr 2.64e-05 | (539.58 ms | 7591 tok/s)\n",
      "step  227/1000 | train loss 1.794987 | norm 19.7109 | lr 2.64e-05 | (629.31 ms | 6509 tok/s)\n",
      "step  228/1000 | train loss 1.786958 | norm 19.5058 | lr 2.63e-05 | (560.32 ms | 7310 tok/s)\n",
      "step  229/1000 | train loss 1.785522 | norm 20.3938 | lr 2.63e-05 | (487.44 ms | 8403 tok/s)\n",
      "step  230/1000 | train loss 1.781999 | norm 24.3380 | lr 2.63e-05 | (541.00 ms | 7571 tok/s)\n",
      "step  231/1000 | train loss 1.772352 | norm 16.9970 | lr 2.63e-05 | (581.24 ms | 7047 tok/s)\n",
      "step  232/1000 | train loss 1.771558 | norm 22.3582 | lr 2.62e-05 | (573.40 ms | 7143 tok/s)\n",
      "step  233/1000 | train loss 1.764811 | norm 19.5910 | lr 2.62e-05 | (578.41 ms | 7082 tok/s)\n",
      "step  234/1000 | train loss 1.759949 | norm 17.6241 | lr 2.62e-05 | (655.67 ms | 6247 tok/s)\n",
      "step  235/1000 | train loss 1.757649 | norm 25.2166 | lr 2.61e-05 | (699.97 ms | 5852 tok/s)\n",
      "step  236/1000 | train loss 1.750542 | norm 16.9197 | lr 2.61e-05 | (601.31 ms | 6812 tok/s)\n",
      "step  237/1000 | train loss 1.745364 | norm 16.8267 | lr 2.61e-05 | (544.67 ms | 7520 tok/s)\n",
      "step  238/1000 | train loss 1.745108 | norm 26.3589 | lr 2.60e-05 | (592.34 ms | 6915 tok/s)\n",
      "step  239/1000 | train loss 1.735881 | norm 10.7766 | lr 2.60e-05 | (480.65 ms | 8522 tok/s)\n",
      "step  240/1000 | train loss 1.733616 | norm 16.2441 | lr 2.60e-05 | (595.97 ms | 6873 tok/s)\n",
      "step  241/1000 | train loss 1.730403 | norm 21.9505 | lr 2.59e-05 | (487.63 ms | 8400 tok/s)\n",
      "step  242/1000 | train loss 1.723944 | norm 20.4343 | lr 2.59e-05 | (621.27 ms | 6593 tok/s)\n",
      "step  243/1000 | train loss 1.718497 | norm 17.5460 | lr 2.59e-05 | (500.93 ms | 8177 tok/s)\n",
      "step  244/1000 | train loss 1.711681 | norm 12.9468 | lr 2.58e-05 | (513.89 ms | 7971 tok/s)\n",
      "step  245/1000 | train loss 1.713250 | norm 31.5878 | lr 2.58e-05 | (556.98 ms | 7354 tok/s)\n",
      "step  246/1000 | train loss 1.703265 | norm 11.1636 | lr 2.58e-05 | (577.15 ms | 7097 tok/s)\n",
      "step  247/1000 | train loss 1.701988 | norm 21.3056 | lr 2.57e-05 | (569.51 ms | 7192 tok/s)\n",
      "step  248/1000 | train loss 1.699437 | norm 20.6420 | lr 2.57e-05 | (457.63 ms | 8951 tok/s)\n",
      "step  249/1000 | train loss 1.690239 | norm 17.4088 | lr 2.57e-05 | (483.89 ms | 8465 tok/s)\n",
      "step  250/1000 | train loss 1.684837 | norm 13.2288 | lr 2.56e-05 | (445.45 ms | 9195 tok/s)\n",
      "step  251/1000 | train loss 1.683190 | norm 25.1471 | lr 2.56e-05 | (509.66 ms | 8037 tok/s)\n",
      "step  252/1000 | train loss 1.674691 | norm 20.1131 | lr 2.56e-05 | (511.85 ms | 8002 tok/s)\n",
      "step  253/1000 | train loss 1.673508 | norm 22.5481 | lr 2.55e-05 | (573.37 ms | 7144 tok/s)\n",
      "step  254/1000 | train loss 1.663686 | norm 11.8525 | lr 2.55e-05 | (476.93 ms | 8588 tok/s)\n",
      "step  255/1000 | train loss 1.667692 | norm 31.5730 | lr 2.55e-05 | (517.21 ms | 7919 tok/s)\n",
      "step  256/1000 | train loss 1.660566 | norm 15.9710 | lr 2.54e-05 | (623.97 ms | 6564 tok/s)\n",
      "step  257/1000 | train loss 1.653128 | norm 19.5203 | lr 2.54e-05 | (609.40 ms | 6721 tok/s)\n",
      "step  258/1000 | train loss 1.656621 | norm 29.3991 | lr 2.54e-05 | (573.95 ms | 7137 tok/s)\n",
      "step  259/1000 | train loss 1.648232 | norm 18.7012 | lr 2.53e-05 | (503.39 ms | 8137 tok/s)\n",
      "step  260/1000 | train loss 1.640521 | norm 21.9049 | lr 2.53e-05 | (489.01 ms | 8376 tok/s)\n",
      "step  261/1000 | train loss 1.640066 | norm 24.6284 | lr 2.53e-05 | (498.96 ms | 8209 tok/s)\n",
      "step  262/1000 | train loss 1.638017 | norm 23.5279 | lr 2.52e-05 | (479.71 ms | 8539 tok/s)\n",
      "step  263/1000 | train loss 1.621802 | norm 5.9881 | lr 2.52e-05 | (625.48 ms | 6549 tok/s)\n",
      "step  264/1000 | train loss 1.646743 | norm 35.3099 | lr 2.52e-05 | (722.03 ms | 5673 tok/s)\n",
      "step  265/1000 | train loss 1.648371 | norm 30.7298 | lr 2.51e-05 | (591.66 ms | 6923 tok/s)\n",
      "step  266/1000 | train loss 1.620268 | norm 19.5806 | lr 2.51e-05 | (516.66 ms | 7928 tok/s)\n",
      "step  267/1000 | train loss 1.624233 | norm 35.5493 | lr 2.51e-05 | (599.06 ms | 6837 tok/s)\n",
      "step  268/1000 | train loss 1.628929 | norm 26.3152 | lr 2.50e-05 | (568.45 ms | 7206 tok/s)\n",
      "step  269/1000 | train loss 1.621791 | norm 33.0773 | lr 2.50e-05 | (604.26 ms | 6778 tok/s)\n",
      "step  270/1000 | train loss 1.605619 | norm 17.6073 | lr 2.50e-05 | (477.51 ms | 8578 tok/s)\n",
      "step  271/1000 | train loss 1.599584 | norm 22.0891 | lr 2.49e-05 | (600.43 ms | 6822 tok/s)\n",
      "step  272/1000 | train loss 1.610431 | norm 30.5983 | lr 2.49e-05 | (481.92 ms | 8499 tok/s)\n",
      "step  273/1000 | train loss 1.600577 | norm 20.0363 | lr 2.48e-05 | (490.35 ms | 8353 tok/s)\n",
      "step  274/1000 | train loss 1.590452 | norm 29.8716 | lr 2.48e-05 | (554.76 ms | 7383 tok/s)\n",
      "step  275/1000 | train loss 1.592082 | norm 22.5562 | lr 2.48e-05 | (565.47 ms | 7243 tok/s)\n",
      "step  276/1000 | train loss 1.589396 | norm 32.8113 | lr 2.47e-05 | (480.48 ms | 8525 tok/s)\n",
      "step  277/1000 | train loss 1.578721 | norm 19.7734 | lr 2.47e-05 | (512.25 ms | 7996 tok/s)\n",
      "step  278/1000 | train loss 1.575806 | norm 24.0162 | lr 2.47e-05 | (593.27 ms | 6904 tok/s)\n",
      "step  279/1000 | train loss 1.569756 | norm 19.3973 | lr 2.46e-05 | (538.50 ms | 7606 tok/s)\n",
      "step  280/1000 | train loss 1.572087 | norm 27.9809 | lr 2.46e-05 | (569.63 ms | 7191 tok/s)\n",
      "step  281/1000 | train loss 1.562798 | norm 15.4453 | lr 2.46e-05 | (514.57 ms | 7960 tok/s)\n",
      "step  282/1000 | train loss 1.564880 | norm 34.6852 | lr 2.45e-05 | (488.87 ms | 8379 tok/s)\n",
      "step  283/1000 | train loss 1.559453 | norm 18.4191 | lr 2.45e-05 | (478.91 ms | 8553 tok/s)\n",
      "step  284/1000 | train loss 1.567175 | norm 42.2881 | lr 2.45e-05 | (521.04 ms | 7861 tok/s)\n",
      "step  285/1000 | train loss 1.561787 | norm 32.8983 | lr 2.44e-05 | (491.09 ms | 8341 tok/s)\n",
      "step  286/1000 | train loss 1.554837 | norm 35.9020 | lr 2.44e-05 | (565.06 ms | 7249 tok/s)\n",
      "step  287/1000 | train loss 1.553770 | norm 34.8228 | lr 2.43e-05 | (519.51 ms | 7884 tok/s)\n",
      "step  288/1000 | train loss 1.543382 | norm 26.1675 | lr 2.43e-05 | (603.01 ms | 6793 tok/s)\n",
      "step  289/1000 | train loss 1.543382 | norm 26.2800 | lr 2.43e-05 | (546.81 ms | 7491 tok/s)\n",
      "step  290/1000 | train loss 1.538168 | norm 25.7026 | lr 2.42e-05 | (554.59 ms | 7386 tok/s)\n",
      "step  291/1000 | train loss 1.529249 | norm 18.4442 | lr 2.42e-05 | (570.64 ms | 7178 tok/s)\n",
      "step  292/1000 | train loss 1.529619 | norm 21.5176 | lr 2.42e-05 | (573.90 ms | 7137 tok/s)\n",
      "step  293/1000 | train loss 1.519849 | norm 11.1137 | lr 2.41e-05 | (552.32 ms | 7416 tok/s)\n",
      "step  294/1000 | train loss 1.529257 | norm 41.3018 | lr 2.41e-05 | (549.43 ms | 7455 tok/s)\n",
      "step  295/1000 | train loss 1.518783 | norm 21.4295 | lr 2.40e-05 | (524.63 ms | 7807 tok/s)\n",
      "step  296/1000 | train loss 1.536606 | norm 62.1177 | lr 2.40e-05 | (606.52 ms | 6753 tok/s)\n",
      "step  297/1000 | train loss 1.535227 | norm 54.5763 | lr 2.40e-05 | (549.13 ms | 7459 tok/s)\n",
      "step  298/1000 | train loss 1.512106 | norm 31.9730 | lr 2.39e-05 | (497.41 ms | 8235 tok/s)\n",
      "step  299/1000 | train loss 1.520974 | norm 44.8674 | lr 2.39e-05 | (487.50 ms | 8402 tok/s)\n",
      "step  300/1000 | train loss 1.512801 | norm 33.4571 | lr 2.39e-05 | (474.11 ms | 8639 tok/s)\n",
      "step  301/1000 | train loss 1.511390 | norm 40.3360 | lr 2.38e-05 | (622.58 ms | 6579 tok/s)\n",
      "step  302/1000 | train loss 1.510163 | norm 36.7839 | lr 2.38e-05 | (693.90 ms | 5903 tok/s)\n",
      "step  303/1000 | train loss 1.498739 | norm 31.4510 | lr 2.37e-05 | (566.74 ms | 7227 tok/s)\n",
      "step  304/1000 | train loss 1.498114 | norm 32.3933 | lr 2.37e-05 | (602.34 ms | 6800 tok/s)\n",
      "step  305/1000 | train loss 1.494883 | norm 30.4807 | lr 2.37e-05 | (485.47 ms | 8437 tok/s)\n",
      "step  306/1000 | train loss 1.488693 | norm 30.1871 | lr 2.36e-05 | (469.15 ms | 8731 tok/s)\n",
      "step  307/1000 | train loss 1.489211 | norm 30.2412 | lr 2.36e-05 | (515.52 ms | 7945 tok/s)\n",
      "step  308/1000 | train loss 1.480312 | norm 20.0168 | lr 2.35e-05 | (440.43 ms | 9300 tok/s)\n",
      "step  309/1000 | train loss 1.481642 | norm 32.2028 | lr 2.35e-05 | (562.65 ms | 7280 tok/s)\n",
      "step  310/1000 | train loss 1.479350 | norm 23.9807 | lr 2.35e-05 | (547.15 ms | 7486 tok/s)\n",
      "step  311/1000 | train loss 1.469654 | norm 17.9428 | lr 2.34e-05 | (538.94 ms | 7600 tok/s)\n",
      "step  312/1000 | train loss 1.481689 | norm 48.2526 | lr 2.34e-05 | (535.91 ms | 7643 tok/s)\n",
      "step  313/1000 | train loss 1.474596 | norm 32.0233 | lr 2.34e-05 | (542.42 ms | 7551 tok/s)\n",
      "step  314/1000 | train loss 1.476838 | norm 52.3928 | lr 2.33e-05 | (541.88 ms | 7559 tok/s)\n",
      "step  315/1000 | train loss 1.470627 | norm 36.6817 | lr 2.33e-05 | (528.72 ms | 7747 tok/s)\n",
      "step  316/1000 | train loss 1.479098 | norm 57.3234 | lr 2.32e-05 | (552.50 ms | 7414 tok/s)\n",
      "step  317/1000 | train loss 1.470596 | norm 48.9205 | lr 2.32e-05 | (495.59 ms | 8265 tok/s)\n",
      "step  318/1000 | train loss 1.458007 | norm 33.5237 | lr 2.32e-05 | (484.44 ms | 8455 tok/s)\n",
      "step  319/1000 | train loss 1.456189 | norm 36.8351 | lr 2.31e-05 | (471.91 ms | 8680 tok/s)\n",
      "step  320/1000 | train loss 1.453753 | norm 32.1261 | lr 2.31e-05 | (486.76 ms | 8415 tok/s)\n",
      "step  321/1000 | train loss 1.446523 | norm 26.3546 | lr 2.30e-05 | (451.70 ms | 9068 tok/s)\n",
      "step  322/1000 | train loss 1.448508 | norm 38.0929 | lr 2.30e-05 | (531.35 ms | 7709 tok/s)\n",
      "step  323/1000 | train loss 1.446758 | norm 28.7725 | lr 2.30e-05 | (554.07 ms | 7393 tok/s)\n",
      "step  324/1000 | train loss 1.443858 | norm 36.8707 | lr 2.29e-05 | (581.19 ms | 7048 tok/s)\n",
      "step  325/1000 | train loss 1.436327 | norm 25.6863 | lr 2.29e-05 | (557.16 ms | 7352 tok/s)\n",
      "step  326/1000 | train loss 1.446796 | norm 46.6778 | lr 2.28e-05 | (564.31 ms | 7258 tok/s)\n",
      "step  327/1000 | train loss 1.438781 | norm 39.8619 | lr 2.28e-05 | (572.23 ms | 7158 tok/s)\n",
      "step  328/1000 | train loss 1.431394 | norm 30.8027 | lr 2.28e-05 | (577.10 ms | 7098 tok/s)\n",
      "step  329/1000 | train loss 1.428171 | norm 31.4717 | lr 2.27e-05 | (480.56 ms | 8523 tok/s)\n",
      "step  330/1000 | train loss 1.426266 | norm 29.7597 | lr 2.27e-05 | (524.09 ms | 7815 tok/s)\n",
      "step  331/1000 | train loss 1.418259 | norm 22.4980 | lr 2.26e-05 | (476.82 ms | 8590 tok/s)\n",
      "step  332/1000 | train loss 1.426642 | norm 42.2169 | lr 2.26e-05 | (486.35 ms | 8422 tok/s)\n",
      "step  333/1000 | train loss 1.422827 | norm 36.3549 | lr 2.26e-05 | (525.11 ms | 7800 tok/s)\n",
      "step  334/1000 | train loss 1.412309 | norm 25.8808 | lr 2.25e-05 | (470.05 ms | 8714 tok/s)\n",
      "step  335/1000 | train loss 1.411593 | norm 29.0092 | lr 2.25e-05 | (635.13 ms | 6449 tok/s)\n",
      "step  336/1000 | train loss 1.408167 | norm 25.9631 | lr 2.24e-05 | (592.99 ms | 6907 tok/s)\n",
      "step  337/1000 | train loss 1.402817 | norm 20.8315 | lr 2.24e-05 | (588.53 ms | 6960 tok/s)\n",
      "step  338/1000 | train loss 1.402352 | norm 28.5472 | lr 2.24e-05 | (580.95 ms | 7051 tok/s)\n",
      "step  339/1000 | train loss 1.398363 | norm 19.6191 | lr 2.23e-05 | (661.30 ms | 6194 tok/s)\n",
      "step  340/1000 | train loss 1.398131 | norm 32.7919 | lr 2.23e-05 | (571.80 ms | 7163 tok/s)\n",
      "step  341/1000 | train loss 1.396824 | norm 24.4499 | lr 2.22e-05 | (534.91 ms | 7657 tok/s)\n",
      "step  342/1000 | train loss 1.391038 | norm 27.2362 | lr 2.22e-05 | (517.61 ms | 7913 tok/s)\n",
      "step  343/1000 | train loss 1.388483 | norm 20.6212 | lr 2.21e-05 | (496.96 ms | 8242 tok/s)\n",
      "step  344/1000 | train loss 1.382837 | norm 21.2571 | lr 2.21e-05 | (478.73 ms | 8556 tok/s)\n",
      "step  345/1000 | train loss 1.381660 | norm 18.2964 | lr 2.21e-05 | (531.33 ms | 7709 tok/s)\n",
      "step  346/1000 | train loss 1.375121 | norm 17.5911 | lr 2.20e-05 | (541.84 ms | 7559 tok/s)\n",
      "step  347/1000 | train loss 1.376066 | norm 21.4269 | lr 2.20e-05 | (575.76 ms | 7114 tok/s)\n",
      "step  348/1000 | train loss 1.369300 | norm 21.6607 | lr 2.19e-05 | (563.84 ms | 7264 tok/s)\n",
      "step  349/1000 | train loss 1.374740 | norm 39.1939 | lr 2.19e-05 | (595.73 ms | 6876 tok/s)\n",
      "step  350/1000 | train loss 1.363313 | norm 15.7254 | lr 2.19e-05 | (544.44 ms | 7523 tok/s)\n",
      "step  351/1000 | train loss 1.378761 | norm 48.3983 | lr 2.18e-05 | (530.18 ms | 7726 tok/s)\n",
      "step  352/1000 | train loss 1.380728 | norm 42.9894 | lr 2.18e-05 | (542.30 ms | 7553 tok/s)\n",
      "step  353/1000 | train loss 1.368735 | norm 40.3797 | lr 2.17e-05 | (521.53 ms | 7854 tok/s)\n",
      "step  354/1000 | train loss 1.359200 | norm 31.2277 | lr 2.17e-05 | (515.69 ms | 7943 tok/s)\n",
      "step  355/1000 | train loss 1.357937 | norm 28.3117 | lr 2.16e-05 | (583.83 ms | 7016 tok/s)\n",
      "step  356/1000 | train loss 1.353870 | norm 28.5226 | lr 2.16e-05 | (561.46 ms | 7295 tok/s)\n",
      "step  357/1000 | train loss 1.348008 | norm 22.7620 | lr 2.16e-05 | (485.40 ms | 8438 tok/s)\n",
      "step  358/1000 | train loss 1.348405 | norm 30.7670 | lr 2.15e-05 | (550.13 ms | 7446 tok/s)\n",
      "step  359/1000 | train loss 1.341612 | norm 22.0598 | lr 2.15e-05 | (658.01 ms | 6225 tok/s)\n",
      "step  360/1000 | train loss 1.353231 | norm 56.5370 | lr 2.14e-05 | (635.40 ms | 6446 tok/s)\n",
      "step  361/1000 | train loss 1.347383 | norm 35.9025 | lr 2.14e-05 | (595.77 ms | 6875 tok/s)\n",
      "step  362/1000 | train loss 1.336580 | norm 28.5817 | lr 2.13e-05 | (459.56 ms | 8913 tok/s)\n",
      "step  363/1000 | train loss 1.338905 | norm 30.4345 | lr 2.13e-05 | (515.05 ms | 7953 tok/s)\n",
      "step  364/1000 | train loss 1.332124 | norm 27.0001 | lr 2.13e-05 | (570.64 ms | 7178 tok/s)\n",
      "step  365/1000 | train loss 1.327347 | norm 24.6103 | lr 2.12e-05 | (495.65 ms | 8264 tok/s)\n",
      "step  366/1000 | train loss 1.324987 | norm 19.7961 | lr 2.12e-05 | (514.98 ms | 7954 tok/s)\n",
      "step  367/1000 | train loss 1.324900 | norm 31.7289 | lr 2.11e-05 | (519.59 ms | 7883 tok/s)\n",
      "step  368/1000 | train loss 1.321350 | norm 26.3138 | lr 2.11e-05 | (488.05 ms | 8393 tok/s)\n",
      "step  369/1000 | train loss 1.315335 | norm 21.7556 | lr 2.10e-05 | (541.67 ms | 7562 tok/s)\n",
      "step  370/1000 | train loss 1.314170 | norm 23.8016 | lr 2.10e-05 | (561.49 ms | 7295 tok/s)\n",
      "step  371/1000 | train loss 1.307013 | norm 14.3332 | lr 2.10e-05 | (577.48 ms | 7093 tok/s)\n",
      "step  372/1000 | train loss 1.312843 | norm 31.2200 | lr 2.09e-05 | (572.22 ms | 7158 tok/s)\n",
      "step  373/1000 | train loss 1.309938 | norm 23.9285 | lr 2.09e-05 | (475.10 ms | 8621 tok/s)\n",
      "step  374/1000 | train loss 1.301125 | norm 23.2069 | lr 2.08e-05 | (529.69 ms | 7733 tok/s)\n",
      "step  375/1000 | train loss 1.304023 | norm 25.5293 | lr 2.08e-05 | (508.94 ms | 8048 tok/s)\n",
      "step  376/1000 | train loss 1.296902 | norm 14.9342 | lr 2.07e-05 | (587.85 ms | 6968 tok/s)\n",
      "step  377/1000 | train loss 1.299995 | norm 34.5751 | lr 2.07e-05 | (497.46 ms | 8234 tok/s)\n",
      "step  378/1000 | train loss 1.303779 | norm 25.2775 | lr 2.07e-05 | (578.74 ms | 7077 tok/s)\n",
      "step  379/1000 | train loss 1.293100 | norm 29.6076 | lr 2.06e-05 | (555.63 ms | 7372 tok/s)\n",
      "step  380/1000 | train loss 1.284917 | norm 15.1736 | lr 2.06e-05 | (614.29 ms | 6668 tok/s)\n",
      "step  381/1000 | train loss 1.290360 | norm 40.2988 | lr 2.05e-05 | (561.21 ms | 7299 tok/s)\n",
      "step  382/1000 | train loss 1.283168 | norm 20.1492 | lr 2.05e-05 | (588.75 ms | 6957 tok/s)\n",
      "step  383/1000 | train loss 1.282844 | norm 30.3710 | lr 2.04e-05 | (453.67 ms | 9029 tok/s)\n",
      "step  384/1000 | train loss 1.280768 | norm 25.3124 | lr 2.04e-05 | (542.78 ms | 7546 tok/s)\n",
      "step  385/1000 | train loss 1.275470 | norm 24.4471 | lr 2.03e-05 | (572.31 ms | 7157 tok/s)\n",
      "step  386/1000 | train loss 1.273046 | norm 25.3603 | lr 2.03e-05 | (563.64 ms | 7267 tok/s)\n",
      "step  387/1000 | train loss 1.268793 | norm 17.1414 | lr 2.03e-05 | (527.99 ms | 7758 tok/s)\n",
      "step  388/1000 | train loss 1.270551 | norm 32.4407 | lr 2.02e-05 | (564.53 ms | 7256 tok/s)\n",
      "step  389/1000 | train loss 1.262208 | norm 15.2035 | lr 2.02e-05 | (551.29 ms | 7430 tok/s)\n",
      "step  390/1000 | train loss 1.270990 | norm 34.3367 | lr 2.01e-05 | (511.62 ms | 8006 tok/s)\n",
      "step  391/1000 | train loss 1.264000 | norm 26.1652 | lr 2.01e-05 | (500.86 ms | 8178 tok/s)\n",
      "step  392/1000 | train loss 1.262610 | norm 34.3686 | lr 2.00e-05 | (568.95 ms | 7199 tok/s)\n",
      "step  393/1000 | train loss 1.260117 | norm 28.2928 | lr 2.00e-05 | (510.48 ms | 8024 tok/s)\n",
      "step  394/1000 | train loss 1.253394 | norm 25.4742 | lr 1.99e-05 | (608.13 ms | 6735 tok/s)\n",
      "step  395/1000 | train loss 1.252056 | norm 21.0275 | lr 1.99e-05 | (553.40 ms | 7402 tok/s)\n",
      "step  396/1000 | train loss 1.249128 | norm 27.5746 | lr 1.99e-05 | (609.91 ms | 6716 tok/s)\n",
      "step  397/1000 | train loss 1.243992 | norm 16.8384 | lr 1.98e-05 | (616.73 ms | 6641 tok/s)\n",
      "step  398/1000 | train loss 1.247831 | norm 29.5917 | lr 1.98e-05 | (580.52 ms | 7056 tok/s)\n",
      "step  399/1000 | train loss 1.241273 | norm 21.1892 | lr 1.97e-05 | (472.85 ms | 8662 tok/s)\n",
      "step  400/1000 | train loss 1.243272 | norm 31.7315 | lr 1.97e-05 | (595.13 ms | 6883 tok/s)\n",
      "step  401/1000 | train loss 1.241836 | norm 28.5263 | lr 1.96e-05 | (498.63 ms | 8215 tok/s)\n",
      "step  402/1000 | train loss 1.232217 | norm 19.2561 | lr 1.96e-05 | (539.47 ms | 7593 tok/s)\n",
      "step  403/1000 | train loss 1.232541 | norm 22.1243 | lr 1.95e-05 | (533.12 ms | 7683 tok/s)\n",
      "step  404/1000 | train loss 1.227769 | norm 18.7247 | lr 1.95e-05 | (537.69 ms | 7618 tok/s)\n",
      "step  405/1000 | train loss 1.226129 | norm 22.1676 | lr 1.95e-05 | (580.16 ms | 7060 tok/s)\n",
      "step  406/1000 | train loss 1.225065 | norm 27.3012 | lr 1.94e-05 | (592.17 ms | 6917 tok/s)\n",
      "step  407/1000 | train loss 1.221448 | norm 18.7290 | lr 1.94e-05 | (588.86 ms | 6956 tok/s)\n",
      "step  408/1000 | train loss 1.213939 | norm 10.5068 | lr 1.93e-05 | (566.69 ms | 7228 tok/s)\n",
      "step  409/1000 | train loss 1.219396 | norm 24.4063 | lr 1.93e-05 | (532.35 ms | 7694 tok/s)\n",
      "step  410/1000 | train loss 1.215398 | norm 22.5670 | lr 1.92e-05 | (470.74 ms | 8701 tok/s)\n",
      "step  411/1000 | train loss 1.209100 | norm 17.4996 | lr 1.92e-05 | (548.84 ms | 7463 tok/s)\n",
      "step  412/1000 | train loss 1.208477 | norm 22.1246 | lr 1.91e-05 | (473.94 ms | 8642 tok/s)\n",
      "step  413/1000 | train loss 1.205335 | norm 18.8270 | lr 1.91e-05 | (526.47 ms | 7780 tok/s)\n",
      "step  414/1000 | train loss 1.203244 | norm 24.2953 | lr 1.91e-05 | (590.07 ms | 6942 tok/s)\n",
      "step  415/1000 | train loss 1.197206 | norm 13.2528 | lr 1.90e-05 | (535.81 ms | 7644 tok/s)\n",
      "step  416/1000 | train loss 1.197968 | norm 21.8334 | lr 1.90e-05 | (479.58 ms | 8541 tok/s)\n",
      "step  417/1000 | train loss 1.193130 | norm 15.9995 | lr 1.89e-05 | (512.20 ms | 7997 tok/s)\n",
      "step  418/1000 | train loss 1.192661 | norm 24.2239 | lr 1.89e-05 | (571.18 ms | 7171 tok/s)\n",
      "step  419/1000 | train loss 1.189297 | norm 16.4349 | lr 1.88e-05 | (589.06 ms | 6953 tok/s)\n",
      "step  420/1000 | train loss 1.184140 | norm 11.2900 | lr 1.88e-05 | (556.81 ms | 7356 tok/s)\n",
      "step  421/1000 | train loss 1.182253 | norm 13.6388 | lr 1.87e-05 | (498.87 ms | 8211 tok/s)\n",
      "step  422/1000 | train loss 1.187230 | norm 31.5013 | lr 1.87e-05 | (508.95 ms | 8048 tok/s)\n",
      "step  423/1000 | train loss 1.178761 | norm 16.1354 | lr 1.86e-05 | (581.76 ms | 7041 tok/s)\n",
      "step  424/1000 | train loss 1.200247 | norm 78.1340 | lr 1.86e-05 | (575.55 ms | 7117 tok/s)\n",
      "step  425/1000 | train loss 1.200665 | norm 64.4835 | lr 1.85e-05 | (582.81 ms | 7028 tok/s)\n",
      "step  426/1000 | train loss 1.183311 | norm 28.2502 | lr 1.85e-05 | (579.83 ms | 7064 tok/s)\n",
      "step  427/1000 | train loss 1.178763 | norm 35.7561 | lr 1.85e-05 | (548.35 ms | 7470 tok/s)\n",
      "step  428/1000 | train loss 1.175446 | norm 21.4659 | lr 1.84e-05 | (499.65 ms | 8198 tok/s)\n",
      "step  429/1000 | train loss 1.179113 | norm 50.1721 | lr 1.84e-05 | (491.96 ms | 8326 tok/s)\n",
      "step  430/1000 | train loss 1.171714 | norm 33.3881 | lr 1.83e-05 | (563.23 ms | 7272 tok/s)\n",
      "step  431/1000 | train loss 1.170981 | norm 30.3901 | lr 1.83e-05 | (605.98 ms | 6759 tok/s)\n",
      "step  432/1000 | train loss 1.169659 | norm 32.5243 | lr 1.82e-05 | (608.04 ms | 6736 tok/s)\n",
      "step  433/1000 | train loss 1.157459 | norm 16.5276 | lr 1.82e-05 | (587.95 ms | 6967 tok/s)\n",
      "step  434/1000 | train loss 1.169383 | norm 43.8162 | lr 1.81e-05 | (572.26 ms | 7158 tok/s)\n",
      "step  435/1000 | train loss 1.165556 | norm 32.4006 | lr 1.81e-05 | (527.73 ms | 7762 tok/s)\n",
      "step  436/1000 | train loss 1.155800 | norm 29.5868 | lr 1.80e-05 | (529.33 ms | 7738 tok/s)\n",
      "step  437/1000 | train loss 1.159339 | norm 27.2574 | lr 1.80e-05 | (474.76 ms | 8627 tok/s)\n",
      "step  438/1000 | train loss 1.152330 | norm 29.1806 | lr 1.80e-05 | (510.11 ms | 8030 tok/s)\n",
      "step  439/1000 | train loss 1.151799 | norm 31.3951 | lr 1.79e-05 | (500.66 ms | 8181 tok/s)\n",
      "step  440/1000 | train loss 1.149453 | norm 22.8055 | lr 1.79e-05 | (585.71 ms | 6993 tok/s)\n",
      "step  441/1000 | train loss 1.143879 | norm 18.8455 | lr 1.78e-05 | (499.72 ms | 8197 tok/s)\n",
      "step  442/1000 | train loss 1.147624 | norm 33.2567 | lr 1.78e-05 | (504.71 ms | 8115 tok/s)\n",
      "step  443/1000 | train loss 1.143529 | norm 24.8124 | lr 1.77e-05 | (516.60 ms | 7929 tok/s)\n",
      "step  444/1000 | train loss 1.139856 | norm 23.4581 | lr 1.77e-05 | (566.97 ms | 7224 tok/s)\n",
      "step  445/1000 | train loss 1.138200 | norm 20.9261 | lr 1.76e-05 | (548.97 ms | 7461 tok/s)\n",
      "step  446/1000 | train loss 1.135362 | norm 17.4642 | lr 1.76e-05 | (593.52 ms | 6901 tok/s)\n",
      "step  447/1000 | train loss 1.133811 | norm 24.8399 | lr 1.75e-05 | (550.96 ms | 7434 tok/s)\n",
      "step  448/1000 | train loss 1.133165 | norm 21.6557 | lr 1.75e-05 | (578.99 ms | 7074 tok/s)\n",
      "step  449/1000 | train loss 1.127666 | norm 12.9109 | lr 1.74e-05 | (508.80 ms | 8050 tok/s)\n",
      "step  450/1000 | train loss 1.129228 | norm 26.0191 | lr 1.74e-05 | (541.43 ms | 7565 tok/s)\n",
      "step  451/1000 | train loss 1.132199 | norm 21.0465 | lr 1.73e-05 | (610.23 ms | 6712 tok/s)\n",
      "step  452/1000 | train loss 1.120375 | norm 10.5868 | lr 1.73e-05 | (549.51 ms | 7454 tok/s)\n",
      "step  453/1000 | train loss 1.138206 | norm 53.8622 | lr 1.73e-05 | (587.09 ms | 6977 tok/s)\n",
      "step  454/1000 | train loss 1.146396 | norm 40.7956 | lr 1.72e-05 | (529.61 ms | 7734 tok/s)\n",
      "step  455/1000 | train loss 1.134129 | norm 31.7863 | lr 1.72e-05 | (487.13 ms | 8408 tok/s)\n",
      "step  456/1000 | train loss 1.120623 | norm 24.6203 | lr 1.71e-05 | (532.43 ms | 7693 tok/s)\n",
      "step  457/1000 | train loss 1.121403 | norm 24.9064 | lr 1.71e-05 | (610.36 ms | 6711 tok/s)\n",
      "step  458/1000 | train loss 1.126174 | norm 33.6571 | lr 1.70e-05 | (532.87 ms | 7687 tok/s)\n",
      "step  459/1000 | train loss 1.115166 | norm 17.4480 | lr 1.70e-05 | (624.69 ms | 6557 tok/s)\n",
      "step  460/1000 | train loss 1.110575 | norm 20.2019 | lr 1.69e-05 | (631.00 ms | 6491 tok/s)\n",
      "step  461/1000 | train loss 1.116525 | norm 27.5587 | lr 1.69e-05 | (549.68 ms | 7452 tok/s)\n",
      "step  462/1000 | train loss 1.112800 | norm 20.2280 | lr 1.68e-05 | (561.07 ms | 7300 tok/s)\n",
      "step  463/1000 | train loss 1.105377 | norm 20.6428 | lr 1.68e-05 | (615.36 ms | 6656 tok/s)\n",
      "step  464/1000 | train loss 1.105121 | norm 21.8898 | lr 1.67e-05 | (589.51 ms | 6948 tok/s)\n",
      "step  465/1000 | train loss 1.106019 | norm 18.8795 | lr 1.67e-05 | (546.17 ms | 7500 tok/s)\n",
      "step  466/1000 | train loss 1.097225 | norm 12.0010 | lr 1.66e-05 | (555.47 ms | 7374 tok/s)\n",
      "step  467/1000 | train loss 1.101620 | norm 31.0880 | lr 1.66e-05 | (607.11 ms | 6747 tok/s)\n",
      "step  468/1000 | train loss 1.106821 | norm 21.2895 | lr 1.66e-05 | (532.37 ms | 7694 tok/s)\n",
      "step  469/1000 | train loss 1.100197 | norm 24.4959 | lr 1.65e-05 | (581.74 ms | 7041 tok/s)\n",
      "step  470/1000 | train loss 1.091087 | norm 16.4475 | lr 1.65e-05 | (612.39 ms | 6689 tok/s)\n",
      "step  471/1000 | train loss 1.091606 | norm 16.7884 | lr 1.64e-05 | (595.04 ms | 6884 tok/s)\n",
      "step  472/1000 | train loss 1.092792 | norm 22.0398 | lr 1.64e-05 | (486.81 ms | 8414 tok/s)\n",
      "step  473/1000 | train loss 1.087150 | norm 19.3106 | lr 1.63e-05 | (490.68 ms | 8348 tok/s)\n",
      "step  474/1000 | train loss 1.081405 | norm 10.6972 | lr 1.63e-05 | (495.80 ms | 8261 tok/s)\n",
      "step  475/1000 | train loss 1.082038 | norm 13.9637 | lr 1.62e-05 | (448.68 ms | 9129 tok/s)\n",
      "step  476/1000 | train loss 1.079471 | norm 12.5576 | lr 1.62e-05 | (517.60 ms | 7913 tok/s)\n",
      "step  477/1000 | train loss 1.076159 | norm 12.8952 | lr 1.61e-05 | (542.14 ms | 7555 tok/s)\n",
      "step  478/1000 | train loss 1.075057 | norm 15.7140 | lr 1.61e-05 | (456.19 ms | 8979 tok/s)\n",
      "step  479/1000 | train loss 1.075305 | norm 21.5950 | lr 1.60e-05 | (502.58 ms | 8150 tok/s)\n",
      "step  480/1000 | train loss 1.070661 | norm 16.0632 | lr 1.60e-05 | (574.94 ms | 7124 tok/s)\n",
      "step  481/1000 | train loss 1.068461 | norm 16.4679 | lr 1.59e-05 | (604.89 ms | 6771 tok/s)\n",
      "step  482/1000 | train loss 1.066576 | norm 14.0008 | lr 1.59e-05 | (569.95 ms | 7187 tok/s)\n",
      "step  483/1000 | train loss 1.065710 | norm 19.0215 | lr 1.58e-05 | (530.35 ms | 7723 tok/s)\n",
      "step  484/1000 | train loss 1.061183 | norm 9.4906 | lr 1.58e-05 | (468.58 ms | 8741 tok/s)\n",
      "step  485/1000 | train loss 1.065087 | norm 21.8706 | lr 1.58e-05 | (476.70 ms | 8592 tok/s)\n",
      "step  486/1000 | train loss 1.063161 | norm 18.9781 | lr 1.57e-05 | (495.72 ms | 8263 tok/s)\n",
      "step  487/1000 | train loss 1.057005 | norm 15.3800 | lr 1.57e-05 | (473.53 ms | 8650 tok/s)\n",
      "step  488/1000 | train loss 1.058947 | norm 23.1878 | lr 1.56e-05 | (583.80 ms | 7016 tok/s)\n",
      "step  489/1000 | train loss 1.053411 | norm 14.6863 | lr 1.56e-05 | (530.46 ms | 7722 tok/s)\n",
      "step  490/1000 | train loss 1.059568 | norm 36.9166 | lr 1.55e-05 | (508.57 ms | 8054 tok/s)\n",
      "step  491/1000 | train loss 1.057443 | norm 27.5268 | lr 1.55e-05 | (509.30 ms | 8042 tok/s)\n",
      "step  492/1000 | train loss 1.051166 | norm 28.6371 | lr 1.54e-05 | (583.38 ms | 7021 tok/s)\n",
      "step  493/1000 | train loss 1.049743 | norm 22.5037 | lr 1.54e-05 | (566.67 ms | 7228 tok/s)\n",
      "step  494/1000 | train loss 1.048577 | norm 26.0801 | lr 1.53e-05 | (625.35 ms | 6550 tok/s)\n",
      "step  495/1000 | train loss 1.042905 | norm 16.9547 | lr 1.53e-05 | (477.17 ms | 8584 tok/s)\n",
      "step  496/1000 | train loss 1.044091 | norm 19.8479 | lr 1.52e-05 | (536.05 ms | 7641 tok/s)\n",
      "step  497/1000 | train loss 1.041046 | norm 16.0753 | lr 1.52e-05 | (668.69 ms | 6125 tok/s)\n",
      "step  498/1000 | train loss 1.042288 | norm 31.9745 | lr 1.51e-05 | (577.20 ms | 7096 tok/s)\n",
      "step  499/1000 | train loss 1.041457 | norm 23.4384 | lr 1.51e-05 | (454.78 ms | 9006 tok/s)\n",
      "step  500/1000 | train loss 1.035318 | norm 17.7963 | lr 1.50e-05 | (495.42 ms | 8268 tok/s)\n",
      "step  501/1000 | train loss 1.036082 | norm 21.4037 | lr 1.50e-05 | (490.94 ms | 8343 tok/s)\n",
      "step  502/1000 | train loss 1.035390 | norm 22.5919 | lr 1.50e-05 | (525.22 ms | 7799 tok/s)\n",
      "step  503/1000 | train loss 1.029292 | norm 11.9687 | lr 1.49e-05 | (596.13 ms | 6871 tok/s)\n",
      "step  504/1000 | train loss 1.031298 | norm 19.9475 | lr 1.49e-05 | (629.42 ms | 6508 tok/s)\n",
      "step  505/1000 | train loss 1.029680 | norm 17.1323 | lr 1.48e-05 | (700.71 ms | 5845 tok/s)\n",
      "step  506/1000 | train loss 1.026400 | norm 16.4828 | lr 1.48e-05 | (556.76 ms | 7357 tok/s)\n",
      "step  507/1000 | train loss 1.025423 | norm 16.3091 | lr 1.47e-05 | (504.46 ms | 8120 tok/s)\n",
      "step  508/1000 | train loss 1.023214 | norm 20.3789 | lr 1.47e-05 | (606.33 ms | 6755 tok/s)\n",
      "step  509/1000 | train loss 1.022023 | norm 18.5795 | lr 1.46e-05 | (560.65 ms | 7306 tok/s)\n",
      "step  510/1000 | train loss 1.020075 | norm 15.9626 | lr 1.46e-05 | (505.35 ms | 8105 tok/s)\n",
      "step  511/1000 | train loss 1.014837 | norm 8.2818 | lr 1.45e-05 | (485.20 ms | 8442 tok/s)\n",
      "step  512/1000 | train loss 1.019722 | norm 25.2601 | lr 1.45e-05 | (478.42 ms | 8562 tok/s)\n",
      "step  513/1000 | train loss 1.018343 | norm 18.6529 | lr 1.44e-05 | (520.98 ms | 7862 tok/s)\n",
      "step  514/1000 | train loss 1.011919 | norm 13.3326 | lr 1.44e-05 | (551.86 ms | 7422 tok/s)\n",
      "step  515/1000 | train loss 1.014110 | norm 25.2411 | lr 1.43e-05 | (607.15 ms | 6746 tok/s)\n",
      "step  516/1000 | train loss 1.011942 | norm 13.9493 | lr 1.43e-05 | (603.50 ms | 6787 tok/s)\n",
      "step  517/1000 | train loss 1.007302 | norm 15.6488 | lr 1.42e-05 | (484.64 ms | 8452 tok/s)\n",
      "step  518/1000 | train loss 1.009626 | norm 17.6983 | lr 1.42e-05 | (499.96 ms | 8193 tok/s)\n",
      "step  519/1000 | train loss 1.003869 | norm 13.5595 | lr 1.42e-05 | (585.82 ms | 6992 tok/s)\n",
      "step  520/1000 | train loss 1.004856 | norm 23.2968 | lr 1.41e-05 | (569.26 ms | 7195 tok/s)\n",
      "step  521/1000 | train loss 1.006386 | norm 16.4412 | lr 1.41e-05 | (537.38 ms | 7622 tok/s)\n",
      "step  522/1000 | train loss 1.002091 | norm 26.2114 | lr 1.40e-05 | (508.34 ms | 8058 tok/s)\n",
      "step  523/1000 | train loss 0.997115 | norm 10.1258 | lr 1.40e-05 | (496.15 ms | 8256 tok/s)\n",
      "step  524/1000 | train loss 0.999593 | norm 21.8024 | lr 1.39e-05 | (489.68 ms | 8365 tok/s)\n",
      "step  525/1000 | train loss 0.997193 | norm 16.9566 | lr 1.39e-05 | (574.50 ms | 7130 tok/s)\n",
      "step  526/1000 | train loss 0.994658 | norm 19.1817 | lr 1.38e-05 | (554.90 ms | 7382 tok/s)\n",
      "step  527/1000 | train loss 0.993836 | norm 17.1685 | lr 1.38e-05 | (588.00 ms | 6966 tok/s)\n",
      "step  528/1000 | train loss 0.989618 | norm 13.3668 | lr 1.37e-05 | (463.55 ms | 8836 tok/s)\n",
      "step  529/1000 | train loss 0.991598 | norm 20.4322 | lr 1.37e-05 | (524.98 ms | 7802 tok/s)\n",
      "step  530/1000 | train loss 0.988191 | norm 11.9563 | lr 1.36e-05 | (582.42 ms | 7033 tok/s)\n",
      "step  531/1000 | train loss 0.986334 | norm 16.5308 | lr 1.36e-05 | (593.98 ms | 6896 tok/s)\n",
      "step  532/1000 | train loss 0.986401 | norm 16.5355 | lr 1.35e-05 | (474.65 ms | 8630 tok/s)\n",
      "step  533/1000 | train loss 0.983712 | norm 17.2222 | lr 1.35e-05 | (480.48 ms | 8525 tok/s)\n",
      "step  534/1000 | train loss 0.982745 | norm 18.8628 | lr 1.34e-05 | (508.41 ms | 8056 tok/s)\n",
      "step  535/1000 | train loss 0.980260 | norm 13.5412 | lr 1.34e-05 | (573.64 ms | 7140 tok/s)\n",
      "step  536/1000 | train loss 0.980206 | norm 24.8233 | lr 1.34e-05 | (577.32 ms | 7095 tok/s)\n",
      "step  537/1000 | train loss 0.978192 | norm 15.3355 | lr 1.33e-05 | (564.68 ms | 7254 tok/s)\n",
      "step  538/1000 | train loss 0.977102 | norm 20.6082 | lr 1.33e-05 | (581.93 ms | 7039 tok/s)\n",
      "step  539/1000 | train loss 0.976974 | norm 19.8847 | lr 1.32e-05 | (553.38 ms | 7402 tok/s)\n",
      "step  540/1000 | train loss 0.973250 | norm 15.1020 | lr 1.32e-05 | (502.53 ms | 8151 tok/s)\n",
      "step  541/1000 | train loss 0.970809 | norm 10.5239 | lr 1.31e-05 | (532.85 ms | 7687 tok/s)\n",
      "step  542/1000 | train loss 0.971771 | norm 20.7447 | lr 1.31e-05 | (574.67 ms | 7128 tok/s)\n",
      "step  543/1000 | train loss 0.968558 | norm 13.9660 | lr 1.30e-05 | (517.68 ms | 7912 tok/s)\n",
      "step  544/1000 | train loss 0.969365 | norm 23.1897 | lr 1.30e-05 | (524.52 ms | 7809 tok/s)\n",
      "step  545/1000 | train loss 0.966702 | norm 13.5644 | lr 1.29e-05 | (445.23 ms | 9200 tok/s)\n",
      "step  546/1000 | train loss 0.966361 | norm 19.9816 | lr 1.29e-05 | (526.84 ms | 7775 tok/s)\n",
      "step  547/1000 | train loss 0.967059 | norm 21.7784 | lr 1.28e-05 | (564.71 ms | 7253 tok/s)\n",
      "step  548/1000 | train loss 0.960676 | norm 12.4152 | lr 1.28e-05 | (557.41 ms | 7348 tok/s)\n",
      "step  549/1000 | train loss 0.966690 | norm 33.3374 | lr 1.27e-05 | (534.83 ms | 7659 tok/s)\n",
      "step  550/1000 | train loss 0.966116 | norm 23.2251 | lr 1.27e-05 | (601.29 ms | 6812 tok/s)\n",
      "step  551/1000 | train loss 0.960072 | norm 21.0590 | lr 1.27e-05 | (576.74 ms | 7102 tok/s)\n",
      "step  552/1000 | train loss 0.961463 | norm 29.0510 | lr 1.26e-05 | (607.88 ms | 6738 tok/s)\n",
      "step  553/1000 | train loss 0.959800 | norm 19.5157 | lr 1.26e-05 | (531.59 ms | 7705 tok/s)\n",
      "step  554/1000 | train loss 0.957184 | norm 21.1833 | lr 1.25e-05 | (589.93 ms | 6943 tok/s)\n",
      "step  555/1000 | train loss 0.954933 | norm 14.4646 | lr 1.25e-05 | (560.91 ms | 7302 tok/s)\n",
      "step  556/1000 | train loss 0.956116 | norm 26.8378 | lr 1.24e-05 | (466.69 ms | 8777 tok/s)\n",
      "step  557/1000 | train loss 0.952210 | norm 15.3962 | lr 1.24e-05 | (467.86 ms | 8755 tok/s)\n",
      "step  558/1000 | train loss 0.952497 | norm 21.3759 | lr 1.23e-05 | (551.85 ms | 7422 tok/s)\n",
      "step  559/1000 | train loss 0.952240 | norm 20.6140 | lr 1.23e-05 | (589.20 ms | 6952 tok/s)\n",
      "step  560/1000 | train loss 0.947182 | norm 14.5149 | lr 1.22e-05 | (623.38 ms | 6571 tok/s)\n",
      "step  561/1000 | train loss 0.948933 | norm 19.0782 | lr 1.22e-05 | (539.51 ms | 7592 tok/s)\n",
      "step  562/1000 | train loss 0.946304 | norm 14.1670 | lr 1.21e-05 | (556.03 ms | 7367 tok/s)\n",
      "step  563/1000 | train loss 0.944005 | norm 11.6711 | lr 1.21e-05 | (563.90 ms | 7264 tok/s)\n",
      "step  564/1000 | train loss 0.943140 | norm 14.3886 | lr 1.21e-05 | (569.93 ms | 7187 tok/s)\n",
      "step  565/1000 | train loss 0.941867 | norm 16.1840 | lr 1.20e-05 | (499.86 ms | 8194 tok/s)\n",
      "step  566/1000 | train loss 0.941054 | norm 12.4805 | lr 1.20e-05 | (508.71 ms | 8052 tok/s)\n",
      "step  567/1000 | train loss 0.938508 | norm 13.1962 | lr 1.19e-05 | (572.19 ms | 7158 tok/s)\n",
      "step  568/1000 | train loss 0.937606 | norm 11.1870 | lr 1.19e-05 | (509.08 ms | 8046 tok/s)\n",
      "step  569/1000 | train loss 0.936251 | norm 11.1102 | lr 1.18e-05 | (500.04 ms | 8191 tok/s)\n",
      "step  570/1000 | train loss 0.935596 | norm 15.9615 | lr 1.18e-05 | (464.41 ms | 8820 tok/s)\n",
      "step  571/1000 | train loss 0.933906 | norm 12.7708 | lr 1.17e-05 | (559.46 ms | 7321 tok/s)\n",
      "step  572/1000 | train loss 0.931387 | norm 7.4608 | lr 1.17e-05 | (567.37 ms | 7219 tok/s)\n",
      "step  573/1000 | train loss 0.932433 | norm 15.2321 | lr 1.16e-05 | (607.96 ms | 6737 tok/s)\n",
      "step  574/1000 | train loss 0.930242 | norm 10.6281 | lr 1.16e-05 | (610.80 ms | 6706 tok/s)\n",
      "step  575/1000 | train loss 0.929441 | norm 16.9169 | lr 1.15e-05 | (600.94 ms | 6816 tok/s)\n",
      "step  576/1000 | train loss 0.928805 | norm 15.5511 | lr 1.15e-05 | (485.60 ms | 8435 tok/s)\n",
      "step  577/1000 | train loss 0.925052 | norm 6.5626 | lr 1.15e-05 | (527.60 ms | 7763 tok/s)\n",
      "step  578/1000 | train loss 0.925855 | norm 18.1916 | lr 1.14e-05 | (462.44 ms | 8857 tok/s)\n",
      "step  579/1000 | train loss 0.926443 | norm 13.3689 | lr 1.14e-05 | (529.73 ms | 7732 tok/s)\n",
      "step  580/1000 | train loss 0.921723 | norm 11.3430 | lr 1.13e-05 | (515.72 ms | 7942 tok/s)\n",
      "step  581/1000 | train loss 0.922602 | norm 18.3930 | lr 1.13e-05 | (471.73 ms | 8683 tok/s)\n",
      "step  582/1000 | train loss 0.921573 | norm 11.9374 | lr 1.12e-05 | (541.12 ms | 7570 tok/s)\n",
      "step  583/1000 | train loss 0.920210 | norm 18.1412 | lr 1.12e-05 | (590.77 ms | 6933 tok/s)\n",
      "step  584/1000 | train loss 0.918013 | norm 11.8374 | lr 1.11e-05 | (653.02 ms | 6272 tok/s)\n",
      "step  585/1000 | train loss 0.920172 | norm 29.1425 | lr 1.11e-05 | (666.07 ms | 6149 tok/s)\n",
      "step  586/1000 | train loss 0.916713 | norm 18.4865 | lr 1.10e-05 | (533.02 ms | 7684 tok/s)\n",
      "step  587/1000 | train loss 0.916031 | norm 16.6495 | lr 1.10e-05 | (530.31 ms | 7724 tok/s)\n",
      "step  588/1000 | train loss 0.914481 | norm 17.1098 | lr 1.10e-05 | (509.51 ms | 8039 tok/s)\n",
      "step  589/1000 | train loss 0.911665 | norm 12.1027 | lr 1.09e-05 | (563.69 ms | 7266 tok/s)\n",
      "step  590/1000 | train loss 0.911025 | norm 19.5363 | lr 1.09e-05 | (512.92 ms | 7986 tok/s)\n",
      "step  591/1000 | train loss 0.908869 | norm 9.7372 | lr 1.08e-05 | (528.46 ms | 7751 tok/s)\n",
      "step  592/1000 | train loss 0.907271 | norm 8.6544 | lr 1.08e-05 | (550.45 ms | 7441 tok/s)\n",
      "step  593/1000 | train loss 0.909800 | norm 27.2052 | lr 1.07e-05 | (595.16 ms | 6882 tok/s)\n",
      "step  594/1000 | train loss 0.906657 | norm 13.9078 | lr 1.07e-05 | (552.24 ms | 7417 tok/s)\n",
      "step  595/1000 | train loss 0.910131 | norm 28.4027 | lr 1.06e-05 | (597.76 ms | 6852 tok/s)\n",
      "step  596/1000 | train loss 0.908548 | norm 24.7063 | lr 1.06e-05 | (593.93 ms | 6896 tok/s)\n",
      "step  597/1000 | train loss 0.904847 | norm 21.9696 | lr 1.05e-05 | (583.04 ms | 7025 tok/s)\n",
      "step  598/1000 | train loss 0.905392 | norm 26.2156 | lr 1.05e-05 | (530.00 ms | 7728 tok/s)\n",
      "step  599/1000 | train loss 0.902208 | norm 21.4596 | lr 1.05e-05 | (579.98 ms | 7062 tok/s)\n",
      "step  600/1000 | train loss 0.900825 | norm 16.1044 | lr 1.04e-05 | (555.39 ms | 7375 tok/s)\n",
      "step  601/1000 | train loss 0.900910 | norm 20.6916 | lr 1.04e-05 | (497.71 ms | 8230 tok/s)\n",
      "step  602/1000 | train loss 0.898619 | norm 14.3565 | lr 1.03e-05 | (487.26 ms | 8406 tok/s)\n",
      "step  603/1000 | train loss 0.898965 | norm 23.3012 | lr 1.03e-05 | (509.47 ms | 8040 tok/s)\n",
      "step  604/1000 | train loss 0.898050 | norm 18.3100 | lr 1.02e-05 | (634.29 ms | 6458 tok/s)\n",
      "step  605/1000 | train loss 0.895533 | norm 11.6530 | lr 1.02e-05 | (590.93 ms | 6931 tok/s)\n",
      "step  606/1000 | train loss 0.894057 | norm 11.8418 | lr 1.01e-05 | (576.83 ms | 7101 tok/s)\n",
      "step  607/1000 | train loss 0.895044 | norm 18.4922 | lr 1.01e-05 | (650.28 ms | 6299 tok/s)\n",
      "step  608/1000 | train loss 0.893124 | norm 14.9530 | lr 1.01e-05 | (637.86 ms | 6421 tok/s)\n",
      "step  609/1000 | train loss 0.892032 | norm 13.6775 | lr 1.00e-05 | (524.32 ms | 7812 tok/s)\n",
      "step  610/1000 | train loss 0.890572 | norm 12.5974 | lr 9.97e-06 | (487.78 ms | 8397 tok/s)\n",
      "step  611/1000 | train loss 0.890372 | norm 16.7714 | lr 9.92e-06 | (540.85 ms | 7573 tok/s)\n",
      "step  612/1000 | train loss 0.889179 | norm 14.1483 | lr 9.88e-06 | (486.82 ms | 8414 tok/s)\n",
      "step  613/1000 | train loss 0.887179 | norm 10.8757 | lr 9.83e-06 | (518.70 ms | 7897 tok/s)\n",
      "step  614/1000 | train loss 0.886517 | norm 10.8069 | lr 9.79e-06 | (562.55 ms | 7281 tok/s)\n",
      "step  615/1000 | train loss 0.885961 | norm 14.9558 | lr 9.74e-06 | (535.22 ms | 7653 tok/s)\n",
      "step  616/1000 | train loss 0.884926 | norm 14.7921 | lr 9.70e-06 | (589.09 ms | 6953 tok/s)\n",
      "step  617/1000 | train loss 0.882679 | norm 7.4932 | lr 9.66e-06 | (590.65 ms | 6935 tok/s)\n",
      "step  618/1000 | train loss 0.881119 | norm 5.8485 | lr 9.61e-06 | (612.20 ms | 6691 tok/s)\n",
      "step  619/1000 | train loss 0.883805 | norm 18.2810 | lr 9.57e-06 | (565.34 ms | 7245 tok/s)\n",
      "step  620/1000 | train loss 0.881237 | norm 12.3745 | lr 9.52e-06 | (563.79 ms | 7265 tok/s)\n",
      "step  621/1000 | train loss 0.880423 | norm 15.2909 | lr 9.48e-06 | (484.28 ms | 8458 tok/s)\n",
      "step  622/1000 | train loss 0.880767 | norm 16.8838 | lr 9.44e-06 | (551.72 ms | 7424 tok/s)\n",
      "step  623/1000 | train loss 0.876432 | norm 7.4260 | lr 9.39e-06 | (437.95 ms | 9353 tok/s)\n",
      "step  624/1000 | train loss 0.877864 | norm 17.7142 | lr 9.35e-06 | (641.96 ms | 6380 tok/s)\n",
      "step  625/1000 | train loss 0.876266 | norm 12.4505 | lr 9.31e-06 | (570.13 ms | 7184 tok/s)\n",
      "step  626/1000 | train loss 0.874613 | norm 11.7705 | lr 9.26e-06 | (519.37 ms | 7887 tok/s)\n",
      "step  627/1000 | train loss 0.873705 | norm 11.2389 | lr 9.22e-06 | (486.78 ms | 8414 tok/s)\n",
      "step  628/1000 | train loss 0.872519 | norm 12.1596 | lr 9.17e-06 | (560.15 ms | 7312 tok/s)\n",
      "step  629/1000 | train loss 0.873079 | norm 18.4890 | lr 9.13e-06 | (540.23 ms | 7582 tok/s)\n",
      "step  630/1000 | train loss 0.870273 | norm 10.2217 | lr 9.09e-06 | (595.23 ms | 6881 tok/s)\n",
      "step  631/1000 | train loss 0.870097 | norm 14.0563 | lr 9.04e-06 | (524.32 ms | 7812 tok/s)\n",
      "step  632/1000 | train loss 0.870570 | norm 17.8168 | lr 9.00e-06 | (610.43 ms | 6710 tok/s)\n",
      "step  633/1000 | train loss 0.866525 | norm 6.1205 | lr 8.96e-06 | (602.45 ms | 6799 tok/s)\n",
      "step  634/1000 | train loss 0.869215 | norm 19.6432 | lr 8.92e-06 | (577.27 ms | 7095 tok/s)\n",
      "step  635/1000 | train loss 0.870732 | norm 20.4475 | lr 8.87e-06 | (475.04 ms | 8622 tok/s)\n",
      "step  636/1000 | train loss 0.865257 | norm 9.2765 | lr 8.83e-06 | (487.17 ms | 8408 tok/s)\n",
      "step  637/1000 | train loss 0.866661 | norm 19.7849 | lr 8.79e-06 | (506.19 ms | 8092 tok/s)\n",
      "step  638/1000 | train loss 0.870252 | norm 18.5943 | lr 8.74e-06 | (536.07 ms | 7641 tok/s)\n",
      "step  639/1000 | train loss 0.866067 | norm 17.1554 | lr 8.70e-06 | (473.75 ms | 8646 tok/s)\n",
      "step  640/1000 | train loss 0.860885 | norm 6.4640 | lr 8.66e-06 | (507.58 ms | 8070 tok/s)\n",
      "step  641/1000 | train loss 0.863154 | norm 14.9522 | lr 8.62e-06 | (537.28 ms | 7624 tok/s)\n",
      "step  642/1000 | train loss 0.863927 | norm 13.1540 | lr 8.57e-06 | (568.44 ms | 7206 tok/s)\n",
      "step  643/1000 | train loss 0.859534 | norm 9.8274 | lr 8.53e-06 | (580.73 ms | 7053 tok/s)\n",
      "step  644/1000 | train loss 0.858877 | norm 10.4395 | lr 8.49e-06 | (674.31 ms | 6074 tok/s)\n",
      "step  645/1000 | train loss 0.859730 | norm 11.0865 | lr 8.45e-06 | (607.68 ms | 6740 tok/s)\n",
      "step  646/1000 | train loss 0.856489 | norm 7.7283 | lr 8.40e-06 | (481.94 ms | 8499 tok/s)\n",
      "step  647/1000 | train loss 0.856654 | norm 12.4923 | lr 8.36e-06 | (532.13 ms | 7697 tok/s)\n",
      "step  648/1000 | train loss 0.857558 | norm 10.7090 | lr 8.32e-06 | (553.53 ms | 7400 tok/s)\n",
      "step  649/1000 | train loss 0.855072 | norm 13.1015 | lr 8.28e-06 | (498.60 ms | 8215 tok/s)\n",
      "step  650/1000 | train loss 0.852946 | norm 7.0415 | lr 8.23e-06 | (541.49 ms | 7564 tok/s)\n",
      "step  651/1000 | train loss 0.852918 | norm 10.3265 | lr 8.19e-06 | (631.46 ms | 6487 tok/s)\n",
      "step  652/1000 | train loss 0.852470 | norm 10.4633 | lr 8.15e-06 | (538.54 ms | 7606 tok/s)\n",
      "step  653/1000 | train loss 0.850675 | norm 9.1564 | lr 8.11e-06 | (628.90 ms | 6513 tok/s)\n",
      "step  654/1000 | train loss 0.849767 | norm 10.9823 | lr 8.07e-06 | (623.46 ms | 6570 tok/s)\n",
      "step  655/1000 | train loss 0.849496 | norm 9.6147 | lr 8.02e-06 | (590.73 ms | 6934 tok/s)\n",
      "step  656/1000 | train loss 0.847873 | norm 7.5411 | lr 7.98e-06 | (538.29 ms | 7609 tok/s)\n",
      "step  657/1000 | train loss 0.847441 | norm 12.1124 | lr 7.94e-06 | (479.49 ms | 8542 tok/s)\n",
      "step  658/1000 | train loss 0.846745 | norm 7.0953 | lr 7.90e-06 | (500.05 ms | 8191 tok/s)\n",
      "step  659/1000 | train loss 0.845408 | norm 9.7322 | lr 7.86e-06 | (476.95 ms | 8588 tok/s)\n",
      "step  660/1000 | train loss 0.844378 | norm 6.0169 | lr 7.82e-06 | (465.29 ms | 8803 tok/s)\n",
      "step  661/1000 | train loss 0.843823 | norm 10.4036 | lr 7.78e-06 | (474.98 ms | 8624 tok/s)\n",
      "step  662/1000 | train loss 0.843219 | norm 11.7127 | lr 7.73e-06 | (492.91 ms | 8310 tok/s)\n",
      "step  663/1000 | train loss 0.841927 | norm 7.2072 | lr 7.69e-06 | (566.11 ms | 7235 tok/s)\n",
      "step  664/1000 | train loss 0.840777 | norm 8.6772 | lr 7.65e-06 | (578.19 ms | 7084 tok/s)\n",
      "step  665/1000 | train loss 0.840196 | norm 9.7736 | lr 7.61e-06 | (609.76 ms | 6717 tok/s)\n",
      "step  666/1000 | train loss 0.839566 | norm 8.9326 | lr 7.57e-06 | (608.92 ms | 6727 tok/s)\n",
      "step  667/1000 | train loss 0.838361 | norm 8.8382 | lr 7.53e-06 | (568.08 ms | 7210 tok/s)\n",
      "step  668/1000 | train loss 0.837192 | norm 6.2841 | lr 7.49e-06 | (499.87 ms | 8194 tok/s)\n",
      "step  669/1000 | train loss 0.836427 | norm 8.1409 | lr 7.45e-06 | (528.64 ms | 7748 tok/s)\n",
      "step  670/1000 | train loss 0.836084 | norm 11.3782 | lr 7.41e-06 | (584.61 ms | 7006 tok/s)\n",
      "step  671/1000 | train loss 0.834509 | norm 5.2042 | lr 7.37e-06 | (522.15 ms | 7845 tok/s)\n",
      "step  672/1000 | train loss 0.834097 | norm 9.6204 | lr 7.33e-06 | (498.13 ms | 8223 tok/s)\n",
      "step  673/1000 | train loss 0.833355 | norm 7.3284 | lr 7.29e-06 | (487.58 ms | 8401 tok/s)\n",
      "step  674/1000 | train loss 0.832173 | norm 6.8245 | lr 7.25e-06 | (537.46 ms | 7621 tok/s)\n",
      "step  675/1000 | train loss 0.831438 | norm 9.3980 | lr 7.21e-06 | (564.32 ms | 7258 tok/s)\n",
      "step  676/1000 | train loss 0.830533 | norm 6.0695 | lr 7.16e-06 | (612.15 ms | 6691 tok/s)\n",
      "step  677/1000 | train loss 0.829681 | norm 7.4828 | lr 7.12e-06 | (519.95 ms | 7878 tok/s)\n",
      "step  678/1000 | train loss 0.828902 | norm 9.5695 | lr 7.08e-06 | (672.40 ms | 6092 tok/s)\n",
      "step  679/1000 | train loss 0.827787 | norm 5.4913 | lr 7.04e-06 | (516.78 ms | 7926 tok/s)\n",
      "step  680/1000 | train loss 0.827188 | norm 7.5087 | lr 7.00e-06 | (500.51 ms | 8184 tok/s)\n",
      "step  681/1000 | train loss 0.826107 | norm 6.6435 | lr 6.96e-06 | (446.33 ms | 9177 tok/s)\n",
      "step  682/1000 | train loss 0.825738 | norm 9.4187 | lr 6.93e-06 | (486.30 ms | 8423 tok/s)\n",
      "step  683/1000 | train loss 0.824489 | norm 5.7447 | lr 6.89e-06 | (470.67 ms | 8703 tok/s)\n",
      "step  684/1000 | train loss 0.823841 | norm 9.0412 | lr 6.85e-06 | (479.75 ms | 8538 tok/s)\n",
      "step  685/1000 | train loss 0.823352 | norm 8.1529 | lr 6.81e-06 | (549.76 ms | 7451 tok/s)\n",
      "step  686/1000 | train loss 0.821782 | norm 4.1188 | lr 6.77e-06 | (654.55 ms | 6258 tok/s)\n",
      "step  687/1000 | train loss 0.821276 | norm 7.1938 | lr 6.73e-06 | (704.82 ms | 5811 tok/s)\n",
      "step  688/1000 | train loss 0.820937 | norm 9.5186 | lr 6.69e-06 | (573.71 ms | 7140 tok/s)\n",
      "step  689/1000 | train loss 0.819591 | norm 5.7202 | lr 6.65e-06 | (534.84 ms | 7658 tok/s)\n",
      "step  690/1000 | train loss 0.819259 | norm 10.8979 | lr 6.61e-06 | (511.83 ms | 8003 tok/s)\n",
      "step  691/1000 | train loss 0.818489 | norm 7.3890 | lr 6.57e-06 | (475.81 ms | 8609 tok/s)\n",
      "step  692/1000 | train loss 0.817432 | norm 6.9220 | lr 6.53e-06 | (530.40 ms | 7723 tok/s)\n",
      "step  693/1000 | train loss 0.816764 | norm 8.7951 | lr 6.49e-06 | (480.36 ms | 8527 tok/s)\n",
      "step  694/1000 | train loss 0.815706 | norm 6.3451 | lr 6.45e-06 | (590.76 ms | 6933 tok/s)\n",
      "step  695/1000 | train loss 0.815077 | norm 6.6327 | lr 6.42e-06 | (626.66 ms | 6536 tok/s)\n",
      "step  696/1000 | train loss 0.814147 | norm 7.2294 | lr 6.38e-06 | (611.97 ms | 6693 tok/s)\n",
      "step  697/1000 | train loss 0.813595 | norm 9.0224 | lr 6.34e-06 | (584.06 ms | 7013 tok/s)\n",
      "step  698/1000 | train loss 0.812566 | norm 5.2692 | lr 6.30e-06 | (599.22 ms | 6836 tok/s)\n",
      "step  699/1000 | train loss 0.812687 | norm 10.3765 | lr 6.26e-06 | (603.91 ms | 6782 tok/s)\n",
      "step  700/1000 | train loss 0.811367 | norm 6.6895 | lr 6.22e-06 | (609.96 ms | 6715 tok/s)\n",
      "step  701/1000 | train loss 0.811137 | norm 9.9543 | lr 6.19e-06 | (509.83 ms | 8034 tok/s)\n",
      "step  702/1000 | train loss 0.810474 | norm 8.7514 | lr 6.15e-06 | (493.01 ms | 8308 tok/s)\n",
      "step  703/1000 | train loss 0.809093 | norm 7.3339 | lr 6.11e-06 | (533.70 ms | 7675 tok/s)\n",
      "step  704/1000 | train loss 0.808837 | norm 8.7983 | lr 6.07e-06 | (495.47 ms | 8267 tok/s)\n",
      "step  705/1000 | train loss 0.808092 | norm 7.3715 | lr 6.03e-06 | (512.05 ms | 7999 tok/s)\n",
      "step  706/1000 | train loss 0.806973 | norm 7.8446 | lr 6.00e-06 | (447.48 ms | 9153 tok/s)\n",
      "step  707/1000 | train loss 0.806665 | norm 7.2756 | lr 5.96e-06 | (551.81 ms | 7423 tok/s)\n",
      "step  708/1000 | train loss 0.805347 | norm 4.9108 | lr 5.92e-06 | (708.73 ms | 5779 tok/s)\n",
      "step  709/1000 | train loss 0.805892 | norm 10.6763 | lr 5.88e-06 | (685.50 ms | 5975 tok/s)\n",
      "step  710/1000 | train loss 0.804671 | norm 7.1483 | lr 5.85e-06 | (677.99 ms | 6041 tok/s)\n",
      "step  711/1000 | train loss 0.804406 | norm 11.2600 | lr 5.81e-06 | (574.22 ms | 7133 tok/s)\n",
      "step  712/1000 | train loss 0.804204 | norm 9.9044 | lr 5.77e-06 | (515.69 ms | 7943 tok/s)\n",
      "step  713/1000 | train loss 0.802974 | norm 10.1845 | lr 5.73e-06 | (508.66 ms | 8052 tok/s)\n",
      "step  714/1000 | train loss 0.802106 | norm 8.8796 | lr 5.70e-06 | (586.75 ms | 6981 tok/s)\n",
      "step  715/1000 | train loss 0.801862 | norm 9.6581 | lr 5.66e-06 | (493.32 ms | 8303 tok/s)\n",
      "step  716/1000 | train loss 0.800939 | norm 8.6526 | lr 5.62e-06 | (607.36 ms | 6744 tok/s)\n",
      "step  717/1000 | train loss 0.800216 | norm 8.2287 | lr 5.59e-06 | (574.70 ms | 7127 tok/s)\n",
      "step  718/1000 | train loss 0.799630 | norm 7.8465 | lr 5.55e-06 | (707.82 ms | 5787 tok/s)\n",
      "step  719/1000 | train loss 0.798826 | norm 6.7200 | lr 5.51e-06 | (580.04 ms | 7062 tok/s)\n",
      "step  720/1000 | train loss 0.798332 | norm 6.6934 | lr 5.48e-06 | (547.49 ms | 7481 tok/s)\n",
      "step  721/1000 | train loss 0.797684 | norm 7.8814 | lr 5.44e-06 | (544.67 ms | 7520 tok/s)\n",
      "step  722/1000 | train loss 0.797285 | norm 8.7693 | lr 5.40e-06 | (514.14 ms | 7967 tok/s)\n",
      "step  723/1000 | train loss 0.796569 | norm 7.4692 | lr 5.37e-06 | (507.10 ms | 8077 tok/s)\n",
      "step  724/1000 | train loss 0.795650 | norm 4.5034 | lr 5.33e-06 | (497.96 ms | 8226 tok/s)\n",
      "step  725/1000 | train loss 0.795441 | norm 6.9727 | lr 5.30e-06 | (537.91 ms | 7615 tok/s)\n",
      "step  726/1000 | train loss 0.794789 | norm 8.2723 | lr 5.26e-06 | (594.16 ms | 6894 tok/s)\n",
      "step  727/1000 | train loss 0.794144 | norm 7.5336 | lr 5.22e-06 | (617.53 ms | 6633 tok/s)\n",
      "step  728/1000 | train loss 0.793294 | norm 4.8570 | lr 5.19e-06 | (505.62 ms | 8101 tok/s)\n",
      "step  729/1000 | train loss 0.793303 | norm 8.9278 | lr 5.15e-06 | (484.98 ms | 8446 tok/s)\n",
      "step  730/1000 | train loss 0.792529 | norm 6.3180 | lr 5.12e-06 | (518.82 ms | 7895 tok/s)\n",
      "step  731/1000 | train loss 0.791863 | norm 6.4384 | lr 5.08e-06 | (611.19 ms | 6702 tok/s)\n",
      "step  732/1000 | train loss 0.791392 | norm 8.5431 | lr 5.05e-06 | (576.80 ms | 7101 tok/s)\n",
      "step  733/1000 | train loss 0.790750 | norm 6.3501 | lr 5.01e-06 | (561.96 ms | 7289 tok/s)\n",
      "step  734/1000 | train loss 0.789885 | norm 4.4317 | lr 4.98e-06 | (533.39 ms | 7679 tok/s)\n",
      "step  735/1000 | train loss 0.790051 | norm 11.2386 | lr 4.94e-06 | (557.00 ms | 7354 tok/s)\n",
      "step  736/1000 | train loss 0.789164 | norm 6.5601 | lr 4.91e-06 | (477.02 ms | 8587 tok/s)\n",
      "step  737/1000 | train loss 0.788938 | norm 10.1062 | lr 4.87e-06 | (509.03 ms | 8047 tok/s)\n",
      "step  738/1000 | train loss 0.788493 | norm 7.9230 | lr 4.84e-06 | (556.22 ms | 7364 tok/s)\n",
      "step  739/1000 | train loss 0.787620 | norm 8.9183 | lr 4.80e-06 | (516.59 ms | 7929 tok/s)\n",
      "step  740/1000 | train loss 0.787378 | norm 8.2701 | lr 4.77e-06 | (467.90 ms | 8754 tok/s)\n",
      "step  741/1000 | train loss 0.786515 | norm 7.4925 | lr 4.73e-06 | (571.30 ms | 7170 tok/s)\n",
      "step  742/1000 | train loss 0.786148 | norm 7.9496 | lr 4.70e-06 | (565.75 ms | 7240 tok/s)\n",
      "step  743/1000 | train loss 0.785528 | norm 5.9053 | lr 4.67e-06 | (618.51 ms | 6622 tok/s)\n",
      "step  744/1000 | train loss 0.785118 | norm 8.1351 | lr 4.63e-06 | (592.50 ms | 6913 tok/s)\n",
      "step  745/1000 | train loss 0.784758 | norm 6.6958 | lr 4.60e-06 | (579.54 ms | 7068 tok/s)\n",
      "step  746/1000 | train loss 0.784075 | norm 8.5585 | lr 4.56e-06 | (549.16 ms | 7459 tok/s)\n",
      "step  747/1000 | train loss 0.783680 | norm 6.4928 | lr 4.53e-06 | (669.59 ms | 6117 tok/s)\n",
      "step  748/1000 | train loss 0.783072 | norm 6.3858 | lr 4.50e-06 | (547.94 ms | 7475 tok/s)\n",
      "step  749/1000 | train loss 0.782608 | norm 7.0359 | lr 4.46e-06 | (534.22 ms | 7667 tok/s)\n",
      "step  750/1000 | train loss 0.781933 | norm 4.5340 | lr 4.43e-06 | (568.10 ms | 7210 tok/s)\n",
      "step  751/1000 | train loss 0.781711 | norm 7.7378 | lr 4.40e-06 | (519.95 ms | 7878 tok/s)\n",
      "step  752/1000 | train loss 0.781208 | norm 5.2743 | lr 4.36e-06 | (593.38 ms | 6903 tok/s)\n",
      "step  753/1000 | train loss 0.780629 | norm 7.5652 | lr 4.33e-06 | (579.65 ms | 7066 tok/s)\n",
      "step  754/1000 | train loss 0.780316 | norm 6.3586 | lr 4.30e-06 | (573.97 ms | 7136 tok/s)\n",
      "step  755/1000 | train loss 0.779563 | norm 5.7283 | lr 4.26e-06 | (601.27 ms | 6812 tok/s)\n",
      "step  756/1000 | train loss 0.779302 | norm 6.9963 | lr 4.23e-06 | (573.02 ms | 7148 tok/s)\n",
      "step  757/1000 | train loss 0.778702 | norm 5.0087 | lr 4.20e-06 | (556.62 ms | 7359 tok/s)\n",
      "step  758/1000 | train loss 0.778302 | norm 7.6622 | lr 4.17e-06 | (569.83 ms | 7188 tok/s)\n",
      "step  759/1000 | train loss 0.777997 | norm 6.0506 | lr 4.13e-06 | (581.22 ms | 7047 tok/s)\n",
      "step  760/1000 | train loss 0.777232 | norm 6.8709 | lr 4.10e-06 | (542.30 ms | 7553 tok/s)\n",
      "step  761/1000 | train loss 0.776874 | norm 5.0923 | lr 4.07e-06 | (494.22 ms | 8288 tok/s)\n",
      "step  762/1000 | train loss 0.776293 | norm 5.8705 | lr 4.04e-06 | (538.68 ms | 7604 tok/s)\n",
      "step  763/1000 | train loss 0.775745 | norm 4.7078 | lr 4.00e-06 | (452.06 ms | 9061 tok/s)\n",
      "step  764/1000 | train loss 0.775273 | norm 5.1081 | lr 3.97e-06 | (518.75 ms | 7896 tok/s)\n",
      "step  765/1000 | train loss 0.774941 | norm 7.6441 | lr 3.94e-06 | (512.41 ms | 7994 tok/s)\n",
      "step  766/1000 | train loss 0.774457 | norm 5.5913 | lr 3.91e-06 | (512.55 ms | 7991 tok/s)\n",
      "step  767/1000 | train loss 0.773825 | norm 4.7419 | lr 3.88e-06 | (534.14 ms | 7668 tok/s)\n",
      "step  768/1000 | train loss 0.773354 | norm 4.1411 | lr 3.84e-06 | (536.88 ms | 7629 tok/s)\n",
      "step  769/1000 | train loss 0.772798 | norm 3.4164 | lr 3.81e-06 | (552.06 ms | 7419 tok/s)\n",
      "step  770/1000 | train loss 0.772681 | norm 7.5068 | lr 3.78e-06 | (542.46 ms | 7551 tok/s)\n",
      "step  771/1000 | train loss 0.772116 | norm 4.9950 | lr 3.75e-06 | (555.90 ms | 7368 tok/s)\n",
      "step  772/1000 | train loss 0.771326 | norm 3.4627 | lr 3.72e-06 | (475.62 ms | 8612 tok/s)\n",
      "step  773/1000 | train loss 0.771700 | norm 10.9030 | lr 3.69e-06 | (482.91 ms | 8482 tok/s)\n",
      "step  774/1000 | train loss 0.771193 | norm 8.4080 | lr 3.66e-06 | (485.09 ms | 8444 tok/s)\n",
      "step  775/1000 | train loss 0.770563 | norm 10.2363 | lr 3.63e-06 | (508.83 ms | 8050 tok/s)\n",
      "step  776/1000 | train loss 0.769863 | norm 7.4639 | lr 3.60e-06 | (451.91 ms | 9064 tok/s)\n",
      "step  777/1000 | train loss 0.769860 | norm 9.4660 | lr 3.57e-06 | (540.29 ms | 7581 tok/s)\n",
      "step  778/1000 | train loss 0.769231 | norm 6.7967 | lr 3.54e-06 | (532.77 ms | 7688 tok/s)\n",
      "step  779/1000 | train loss 0.768903 | norm 10.2715 | lr 3.51e-06 | (559.04 ms | 7327 tok/s)\n",
      "step  780/1000 | train loss 0.768584 | norm 8.6878 | lr 3.48e-06 | (528.48 ms | 7751 tok/s)\n",
      "step  781/1000 | train loss 0.768176 | norm 9.7618 | lr 3.44e-06 | (561.84 ms | 7290 tok/s)\n",
      "step  782/1000 | train loss 0.767608 | norm 7.0679 | lr 3.41e-06 | (619.83 ms | 6608 tok/s)\n",
      "step  783/1000 | train loss 0.767577 | norm 10.5380 | lr 3.39e-06 | (580.59 ms | 7055 tok/s)\n",
      "step  784/1000 | train loss 0.767159 | norm 9.3802 | lr 3.36e-06 | (443.04 ms | 9245 tok/s)\n",
      "step  785/1000 | train loss 0.766598 | norm 7.8487 | lr 3.33e-06 | (530.45 ms | 7722 tok/s)\n",
      "step  786/1000 | train loss 0.766193 | norm 6.9728 | lr 3.30e-06 | (632.92 ms | 6472 tok/s)\n",
      "step  787/1000 | train loss 0.766106 | norm 9.1039 | lr 3.27e-06 | (539.42 ms | 7593 tok/s)\n",
      "step  788/1000 | train loss 0.765583 | norm 6.8948 | lr 3.24e-06 | (477.71 ms | 8574 tok/s)\n",
      "step  789/1000 | train loss 0.765331 | norm 8.7544 | lr 3.21e-06 | (561.31 ms | 7297 tok/s)\n",
      "step  790/1000 | train loss 0.765069 | norm 8.0718 | lr 3.18e-06 | (487.06 ms | 8410 tok/s)\n",
      "step  791/1000 | train loss 0.764653 | norm 8.0689 | lr 3.15e-06 | (552.88 ms | 7408 tok/s)\n",
      "step  792/1000 | train loss 0.764308 | norm 6.9684 | lr 3.12e-06 | (610.14 ms | 6713 tok/s)\n",
      "step  793/1000 | train loss 0.763955 | norm 6.9223 | lr 3.09e-06 | (650.14 ms | 6300 tok/s)\n",
      "step  794/1000 | train loss 0.763656 | norm 6.4600 | lr 3.06e-06 | (564.48 ms | 7256 tok/s)\n",
      "step  795/1000 | train loss 0.763300 | norm 7.1105 | lr 3.04e-06 | (468.59 ms | 8741 tok/s)\n",
      "step  796/1000 | train loss 0.763020 | norm 5.5237 | lr 3.01e-06 | (573.00 ms | 7148 tok/s)\n",
      "step  797/1000 | train loss 0.762590 | norm 5.2867 | lr 2.98e-06 | (696.12 ms | 5884 tok/s)\n",
      "step  798/1000 | train loss 0.762355 | norm 5.7802 | lr 2.95e-06 | (538.96 ms | 7600 tok/s)\n",
      "step  799/1000 | train loss 0.761889 | norm 3.5691 | lr 2.92e-06 | (500.88 ms | 8178 tok/s)\n",
      "step  800/1000 | train loss 0.761764 | norm 6.8932 | lr 2.90e-06 | (508.16 ms | 8060 tok/s)\n",
      "step  801/1000 | train loss 0.761285 | norm 3.7298 | lr 2.87e-06 | (493.32 ms | 8303 tok/s)\n",
      "step  802/1000 | train loss 0.761399 | norm 10.7979 | lr 2.84e-06 | (611.90 ms | 6694 tok/s)\n",
      "step  803/1000 | train loss 0.760986 | norm 8.4728 | lr 2.81e-06 | (614.77 ms | 6663 tok/s)\n",
      "step  804/1000 | train loss 0.760566 | norm 7.0748 | lr 2.78e-06 | (574.68 ms | 7127 tok/s)\n",
      "step  805/1000 | train loss 0.760209 | norm 7.2156 | lr 2.76e-06 | (524.88 ms | 7804 tok/s)\n",
      "step  806/1000 | train loss 0.759762 | norm 5.0056 | lr 2.73e-06 | (596.14 ms | 6871 tok/s)\n",
      "step  807/1000 | train loss 0.759520 | norm 6.6480 | lr 2.70e-06 | (580.16 ms | 7060 tok/s)\n",
      "step  808/1000 | train loss 0.758987 | norm 3.3427 | lr 2.68e-06 | (556.48 ms | 7361 tok/s)\n",
      "step  809/1000 | train loss 0.758942 | norm 6.8281 | lr 2.65e-06 | (483.04 ms | 8480 tok/s)\n",
      "step  810/1000 | train loss 0.758397 | norm 3.5519 | lr 2.62e-06 | (498.47 ms | 8217 tok/s)\n",
      "step  811/1000 | train loss 0.758622 | norm 10.6497 | lr 2.60e-06 | (598.74 ms | 6841 tok/s)\n",
      "step  812/1000 | train loss 0.758432 | norm 10.6836 | lr 2.57e-06 | (555.74 ms | 7370 tok/s)\n",
      "step  813/1000 | train loss 0.757599 | norm 3.6286 | lr 2.54e-06 | (485.94 ms | 8429 tok/s)\n",
      "step  814/1000 | train loss 0.757464 | norm 7.0425 | lr 2.52e-06 | (598.76 ms | 6841 tok/s)\n",
      "step  815/1000 | train loss 0.757288 | norm 5.7684 | lr 2.49e-06 | (536.26 ms | 7638 tok/s)\n",
      "step  816/1000 | train loss 0.756857 | norm 5.7646 | lr 2.47e-06 | (516.85 ms | 7925 tok/s)\n",
      "step  817/1000 | train loss 0.756423 | norm 4.3133 | lr 2.44e-06 | (542.67 ms | 7548 tok/s)\n",
      "step  818/1000 | train loss 0.756322 | norm 5.2002 | lr 2.41e-06 | (680.45 ms | 6020 tok/s)\n",
      "step  819/1000 | train loss 0.755910 | norm 4.3722 | lr 2.39e-06 | (633.46 ms | 6466 tok/s)\n",
      "step  820/1000 | train loss 0.755575 | norm 4.0030 | lr 2.36e-06 | (495.11 ms | 8273 tok/s)\n",
      "step  821/1000 | train loss 0.755329 | norm 4.0401 | lr 2.34e-06 | (453.35 ms | 9035 tok/s)\n",
      "step  822/1000 | train loss 0.754953 | norm 3.5660 | lr 2.31e-06 | (553.66 ms | 7398 tok/s)\n",
      "step  823/1000 | train loss 0.754676 | norm 3.0713 | lr 2.29e-06 | (570.26 ms | 7183 tok/s)\n",
      "step  824/1000 | train loss 0.754382 | norm 3.8649 | lr 2.26e-06 | (538.68 ms | 7604 tok/s)\n",
      "step  825/1000 | train loss 0.754046 | norm 2.8465 | lr 2.24e-06 | (488.61 ms | 8383 tok/s)\n",
      "step  826/1000 | train loss 0.753820 | norm 4.1369 | lr 2.21e-06 | (502.44 ms | 8152 tok/s)\n",
      "step  827/1000 | train loss 0.753458 | norm 2.5871 | lr 2.19e-06 | (513.97 ms | 7969 tok/s)\n",
      "step  828/1000 | train loss 0.753236 | norm 4.0479 | lr 2.16e-06 | (528.03 ms | 7757 tok/s)\n",
      "step  829/1000 | train loss 0.752903 | norm 3.0016 | lr 2.14e-06 | (536.88 ms | 7629 tok/s)\n",
      "step  830/1000 | train loss 0.752666 | norm 4.1777 | lr 2.12e-06 | (671.36 ms | 6101 tok/s)\n",
      "step  831/1000 | train loss 0.752322 | norm 3.5589 | lr 2.09e-06 | (526.68 ms | 7777 tok/s)\n",
      "step  832/1000 | train loss 0.752053 | norm 3.2696 | lr 2.07e-06 | (505.56 ms | 8102 tok/s)\n",
      "step  833/1000 | train loss 0.751722 | norm 3.2034 | lr 2.04e-06 | (549.39 ms | 7456 tok/s)\n",
      "step  834/1000 | train loss 0.751423 | norm 2.6975 | lr 2.02e-06 | (534.02 ms | 7670 tok/s)\n",
      "step  835/1000 | train loss 0.751134 | norm 2.9124 | lr 2.00e-06 | (547.17 ms | 7486 tok/s)\n",
      "step  836/1000 | train loss 0.750830 | norm 2.9186 | lr 1.97e-06 | (495.50 ms | 8266 tok/s)\n",
      "step  837/1000 | train loss 0.750581 | norm 2.9817 | lr 1.95e-06 | (503.43 ms | 8136 tok/s)\n",
      "step  838/1000 | train loss 0.750264 | norm 2.9725 | lr 1.93e-06 | (446.72 ms | 9169 tok/s)\n",
      "step  839/1000 | train loss 0.749990 | norm 2.8833 | lr 1.90e-06 | (598.68 ms | 6842 tok/s)\n",
      "step  840/1000 | train loss 0.749696 | norm 2.2147 | lr 1.88e-06 | (557.65 ms | 7345 tok/s)\n",
      "step  841/1000 | train loss 0.749394 | norm 2.1469 | lr 1.86e-06 | (634.30 ms | 6458 tok/s)\n",
      "step  842/1000 | train loss 0.749196 | norm 4.1610 | lr 1.84e-06 | (518.96 ms | 7893 tok/s)\n",
      "step  843/1000 | train loss 0.748870 | norm 2.3870 | lr 1.81e-06 | (555.88 ms | 7369 tok/s)\n",
      "step  844/1000 | train loss 0.748598 | norm 2.8955 | lr 1.79e-06 | (541.87 ms | 7559 tok/s)\n",
      "step  845/1000 | train loss 0.748386 | norm 3.8754 | lr 1.77e-06 | (559.72 ms | 7318 tok/s)\n",
      "step  846/1000 | train loss 0.748029 | norm 1.7713 | lr 1.75e-06 | (543.96 ms | 7530 tok/s)\n",
      "step  847/1000 | train loss 0.747782 | norm 2.3178 | lr 1.72e-06 | (576.12 ms | 7110 tok/s)\n",
      "step  848/1000 | train loss 0.747560 | norm 3.3131 | lr 1.70e-06 | (492.95 ms | 8309 tok/s)\n",
      "step  849/1000 | train loss 0.747247 | norm 2.1103 | lr 1.68e-06 | (475.32 ms | 8617 tok/s)\n",
      "step  850/1000 | train loss 0.746993 | norm 2.2386 | lr 1.66e-06 | (502.23 ms | 8156 tok/s)\n",
      "step  851/1000 | train loss 0.746768 | norm 3.0501 | lr 1.64e-06 | (486.45 ms | 8420 tok/s)\n",
      "step  852/1000 | train loss 0.746458 | norm 1.9279 | lr 1.62e-06 | (568.66 ms | 7203 tok/s)\n",
      "step  853/1000 | train loss 0.746211 | norm 2.0143 | lr 1.60e-06 | (587.93 ms | 6967 tok/s)\n",
      "step  854/1000 | train loss 0.745992 | norm 3.2186 | lr 1.57e-06 | (521.59 ms | 7853 tok/s)\n",
      "step  855/1000 | train loss 0.745705 | norm 2.1502 | lr 1.55e-06 | (471.55 ms | 8686 tok/s)\n",
      "step  856/1000 | train loss 0.745474 | norm 2.3424 | lr 1.53e-06 | (528.43 ms | 7751 tok/s)\n",
      "step  857/1000 | train loss 0.745210 | norm 1.8671 | lr 1.51e-06 | (549.38 ms | 7456 tok/s)\n",
      "step  858/1000 | train loss 0.744969 | norm 1.9591 | lr 1.49e-06 | (634.36 ms | 6457 tok/s)\n",
      "step  859/1000 | train loss 0.744717 | norm 2.0178 | lr 1.47e-06 | (515.40 ms | 7947 tok/s)\n",
      "step  860/1000 | train loss 0.744500 | norm 2.8305 | lr 1.45e-06 | (593.74 ms | 6899 tok/s)\n",
      "step  861/1000 | train loss 0.744232 | norm 1.6689 | lr 1.43e-06 | (554.31 ms | 7389 tok/s)\n",
      "step  862/1000 | train loss 0.744027 | norm 2.7163 | lr 1.41e-06 | (554.19 ms | 7391 tok/s)\n",
      "step  863/1000 | train loss 0.743768 | norm 1.8369 | lr 1.39e-06 | (491.18 ms | 8339 tok/s)\n",
      "step  864/1000 | train loss 0.743554 | norm 2.2927 | lr 1.37e-06 | (569.00 ms | 7199 tok/s)\n",
      "step  865/1000 | train loss 0.743303 | norm 1.7609 | lr 1.35e-06 | (496.07 ms | 8257 tok/s)\n",
      "step  866/1000 | train loss 0.743100 | norm 2.3368 | lr 1.33e-06 | (477.31 ms | 8581 tok/s)\n",
      "step  867/1000 | train loss 0.742855 | norm 1.8826 | lr 1.31e-06 | (527.48 ms | 7765 tok/s)\n",
      "step  868/1000 | train loss 0.742661 | norm 2.3700 | lr 1.29e-06 | (579.43 ms | 7069 tok/s)\n",
      "step  869/1000 | train loss 0.742419 | norm 1.8644 | lr 1.27e-06 | (543.92 ms | 7530 tok/s)\n",
      "step  870/1000 | train loss 0.742222 | norm 2.2409 | lr 1.26e-06 | (567.25 ms | 7221 tok/s)\n",
      "step  871/1000 | train loss 0.741997 | norm 1.8704 | lr 1.24e-06 | (485.08 ms | 8444 tok/s)\n",
      "step  872/1000 | train loss 0.741794 | norm 1.9771 | lr 1.22e-06 | (477.95 ms | 8570 tok/s)\n",
      "step  873/1000 | train loss 0.741580 | norm 1.8325 | lr 1.20e-06 | (493.99 ms | 8292 tok/s)\n",
      "step  874/1000 | train loss 0.741378 | norm 1.9224 | lr 1.18e-06 | (474.09 ms | 8640 tok/s)\n",
      "step  875/1000 | train loss 0.741171 | norm 1.7977 | lr 1.16e-06 | (546.95 ms | 7489 tok/s)\n",
      "step  876/1000 | train loss 0.740972 | norm 2.0082 | lr 1.14e-06 | (524.47 ms | 7810 tok/s)\n",
      "step  877/1000 | train loss 0.740782 | norm 2.0757 | lr 1.13e-06 | (529.21 ms | 7740 tok/s)\n",
      "step  878/1000 | train loss 0.740570 | norm 1.7604 | lr 1.11e-06 | (590.63 ms | 6935 tok/s)\n",
      "step  879/1000 | train loss 0.740391 | norm 2.0628 | lr 1.09e-06 | (634.39 ms | 6457 tok/s)\n",
      "step  880/1000 | train loss 0.740184 | norm 1.5980 | lr 1.07e-06 | (516.75 ms | 7926 tok/s)\n",
      "step  881/1000 | train loss 0.740006 | norm 1.7976 | lr 1.06e-06 | (587.01 ms | 6978 tok/s)\n",
      "step  882/1000 | train loss 0.739810 | norm 1.6374 | lr 1.04e-06 | (566.45 ms | 7231 tok/s)\n",
      "step  883/1000 | train loss 0.739631 | norm 1.8338 | lr 1.02e-06 | (497.88 ms | 8227 tok/s)\n",
      "step  884/1000 | train loss 0.739444 | norm 1.7328 | lr 1.00e-06 | (442.56 ms | 9255 tok/s)\n",
      "step  885/1000 | train loss 0.739261 | norm 1.6559 | lr 9.88e-07 | (583.41 ms | 7021 tok/s)\n",
      "step  886/1000 | train loss 0.739082 | norm 1.6449 | lr 9.71e-07 | (657.54 ms | 6229 tok/s)\n",
      "step  887/1000 | train loss 0.738900 | norm 1.6026 | lr 9.55e-07 | (625.90 ms | 6544 tok/s)\n",
      "step  888/1000 | train loss 0.738727 | norm 1.6792 | lr 9.38e-07 | (450.37 ms | 9095 tok/s)\n",
      "step  889/1000 | train loss 0.738546 | norm 1.4862 | lr 9.22e-07 | (566.47 ms | 7231 tok/s)\n",
      "step  890/1000 | train loss 0.738375 | norm 1.5248 | lr 9.06e-07 | (623.25 ms | 6572 tok/s)\n",
      "step  891/1000 | train loss 0.738199 | norm 1.4999 | lr 8.90e-07 | (573.38 ms | 7144 tok/s)\n",
      "step  892/1000 | train loss 0.738029 | norm 1.5912 | lr 8.74e-07 | (513.56 ms | 7976 tok/s)\n",
      "step  893/1000 | train loss 0.737859 | norm 1.5969 | lr 8.58e-07 | (566.33 ms | 7233 tok/s)\n",
      "step  894/1000 | train loss 0.737689 | norm 1.5545 | lr 8.42e-07 | (568.35 ms | 7207 tok/s)\n",
      "step  895/1000 | train loss 0.737524 | norm 1.4772 | lr 8.27e-07 | (492.42 ms | 8318 tok/s)\n",
      "step  896/1000 | train loss 0.737354 | norm 1.3367 | lr 8.12e-07 | (461.94 ms | 8867 tok/s)\n",
      "step  897/1000 | train loss 0.737193 | norm 1.3876 | lr 7.96e-07 | (512.65 ms | 7990 tok/s)\n",
      "step  898/1000 | train loss 0.737025 | norm 1.3216 | lr 7.81e-07 | (505.94 ms | 8096 tok/s)\n",
      "step  899/1000 | train loss 0.736865 | norm 1.3766 | lr 7.66e-07 | (451.64 ms | 9069 tok/s)\n",
      "step  900/1000 | train loss 0.736700 | norm 1.3328 | lr 7.52e-07 | (490.59 ms | 8349 tok/s)\n",
      "step  901/1000 | train loss 0.736543 | norm 1.4353 | lr 7.37e-07 | (502.95 ms | 8144 tok/s)\n",
      "step  902/1000 | train loss 0.736383 | norm 1.4425 | lr 7.23e-07 | (590.63 ms | 6935 tok/s)\n",
      "step  903/1000 | train loss 0.736228 | norm 1.5065 | lr 7.08e-07 | (584.09 ms | 7013 tok/s)\n",
      "step  904/1000 | train loss 0.736073 | norm 1.4367 | lr 6.94e-07 | (576.91 ms | 7100 tok/s)\n",
      "step  905/1000 | train loss 0.735922 | norm 1.3652 | lr 6.80e-07 | (549.20 ms | 7458 tok/s)\n",
      "step  906/1000 | train loss 0.735772 | norm 1.3497 | lr 6.66e-07 | (542.70 ms | 7547 tok/s)\n",
      "step  907/1000 | train loss 0.735624 | norm 1.3382 | lr 6.52e-07 | (466.12 ms | 8788 tok/s)\n",
      "step  908/1000 | train loss 0.735479 | norm 1.3461 | lr 6.39e-07 | (466.78 ms | 8775 tok/s)\n",
      "step  909/1000 | train loss 0.735334 | norm 1.3225 | lr 6.25e-07 | (462.61 ms | 8854 tok/s)\n",
      "step  910/1000 | train loss 0.735191 | norm 1.3257 | lr 6.12e-07 | (597.14 ms | 6859 tok/s)\n",
      "step  911/1000 | train loss 0.735050 | norm 1.3359 | lr 5.99e-07 | (511.47 ms | 8008 tok/s)\n",
      "step  912/1000 | train loss 0.734912 | norm 1.3699 | lr 5.85e-07 | (483.28 ms | 8475 tok/s)\n",
      "step  913/1000 | train loss 0.734774 | norm 1.3429 | lr 5.73e-07 | (584.04 ms | 7013 tok/s)\n",
      "step  914/1000 | train loss 0.734639 | norm 1.2975 | lr 5.60e-07 | (565.55 ms | 7243 tok/s)\n",
      "step  915/1000 | train loss 0.734505 | norm 1.2780 | lr 5.47e-07 | (564.61 ms | 7255 tok/s)\n",
      "step  916/1000 | train loss 0.734374 | norm 1.2739 | lr 5.35e-07 | (536.42 ms | 7636 tok/s)\n",
      "step  917/1000 | train loss 0.734245 | norm 1.3019 | lr 5.22e-07 | (580.21 ms | 7060 tok/s)\n",
      "step  918/1000 | train loss 0.734119 | norm 1.3065 | lr 5.10e-07 | (494.91 ms | 8276 tok/s)\n",
      "step  919/1000 | train loss 0.733996 | norm 1.2972 | lr 4.98e-07 | (591.40 ms | 6926 tok/s)\n",
      "step  920/1000 | train loss 0.733875 | norm 1.2795 | lr 4.86e-07 | (527.03 ms | 7772 tok/s)\n",
      "step  921/1000 | train loss 0.733758 | norm 1.2799 | lr 4.74e-07 | (528.96 ms | 7744 tok/s)\n",
      "step  922/1000 | train loss 0.733643 | norm 1.2784 | lr 4.63e-07 | (432.19 ms | 9477 tok/s)\n",
      "step  923/1000 | train loss 0.733532 | norm 1.2738 | lr 4.51e-07 | (495.74 ms | 8262 tok/s)\n",
      "step  924/1000 | train loss 0.733424 | norm 1.2710 | lr 4.40e-07 | (565.23 ms | 7247 tok/s)\n",
      "step  925/1000 | train loss 0.733318 | norm 1.2735 | lr 4.28e-07 | (590.70 ms | 6934 tok/s)\n",
      "step  926/1000 | train loss 0.733216 | norm 1.2776 | lr 4.17e-07 | (559.56 ms | 7320 tok/s)\n",
      "step  927/1000 | train loss 0.733115 | norm 1.2698 | lr 4.06e-07 | (662.00 ms | 6187 tok/s)\n",
      "step  928/1000 | train loss 0.733018 | norm 1.2617 | lr 3.96e-07 | (627.73 ms | 6525 tok/s)\n",
      "step  929/1000 | train loss 0.732923 | norm 1.2647 | lr 3.85e-07 | (618.31 ms | 6625 tok/s)\n",
      "step  930/1000 | train loss 0.732831 | norm 1.2704 | lr 3.75e-07 | (486.43 ms | 8421 tok/s)\n",
      "step  931/1000 | train loss 0.732741 | norm 1.2654 | lr 3.64e-07 | (609.65 ms | 6719 tok/s)\n",
      "step  932/1000 | train loss 0.732654 | norm 1.2604 | lr 3.54e-07 | (562.27 ms | 7285 tok/s)\n",
      "step  933/1000 | train loss 0.732569 | norm 1.2623 | lr 3.44e-07 | (519.67 ms | 7882 tok/s)\n",
      "step  934/1000 | train loss 0.732486 | norm 1.2623 | lr 3.34e-07 | (441.73 ms | 9273 tok/s)\n",
      "step  935/1000 | train loss 0.732406 | norm 1.2614 | lr 3.24e-07 | (581.45 ms | 7044 tok/s)\n",
      "step  936/1000 | train loss 0.732328 | norm 1.2606 | lr 3.15e-07 | (521.79 ms | 7850 tok/s)\n",
      "step  937/1000 | train loss 0.732252 | norm 1.2598 | lr 3.05e-07 | (524.93 ms | 7803 tok/s)\n",
      "step  938/1000 | train loss 0.732178 | norm 1.2594 | lr 2.96e-07 | (534.03 ms | 7670 tok/s)\n",
      "step  939/1000 | train loss 0.732107 | norm 1.2589 | lr 2.87e-07 | (693.80 ms | 5904 tok/s)\n",
      "step  940/1000 | train loss 0.732037 | norm 1.2596 | lr 2.78e-07 | (591.35 ms | 6927 tok/s)\n",
      "step  941/1000 | train loss 0.731970 | norm 1.2581 | lr 2.69e-07 | (578.10 ms | 7085 tok/s)\n",
      "step  942/1000 | train loss 0.731905 | norm 1.2581 | lr 2.60e-07 | (524.16 ms | 7814 tok/s)\n",
      "step  943/1000 | train loss 0.731842 | norm 1.2586 | lr 2.51e-07 | (561.62 ms | 7293 tok/s)\n",
      "step  944/1000 | train loss 0.731780 | norm 1.2572 | lr 2.43e-07 | (468.78 ms | 8737 tok/s)\n",
      "step  945/1000 | train loss 0.731721 | norm 1.2579 | lr 2.35e-07 | (502.54 ms | 8151 tok/s)\n",
      "step  946/1000 | train loss 0.731664 | norm 1.2579 | lr 2.26e-07 | (467.89 ms | 8754 tok/s)\n",
      "step  947/1000 | train loss 0.731608 | norm 1.2569 | lr 2.18e-07 | (500.24 ms | 8188 tok/s)\n",
      "step  948/1000 | train loss 0.731555 | norm 1.2572 | lr 2.10e-07 | (490.17 ms | 8356 tok/s)\n",
      "step  949/1000 | train loss 0.731503 | norm 1.2570 | lr 2.03e-07 | (494.55 ms | 8282 tok/s)\n",
      "step  950/1000 | train loss 0.731453 | norm 1.2569 | lr 1.95e-07 | (543.20 ms | 7540 tok/s)\n",
      "step  951/1000 | train loss 0.731405 | norm 1.2570 | lr 1.88e-07 | (543.75 ms | 7533 tok/s)\n",
      "step  952/1000 | train loss 0.731358 | norm 1.2568 | lr 1.80e-07 | (627.64 ms | 6526 tok/s)\n",
      "step  953/1000 | train loss 0.731313 | norm 1.2567 | lr 1.73e-07 | (567.09 ms | 7223 tok/s)\n",
      "step  954/1000 | train loss 0.731270 | norm 1.2564 | lr 1.66e-07 | (648.47 ms | 6316 tok/s)\n",
      "step  955/1000 | train loss 0.731229 | norm 1.2566 | lr 1.59e-07 | (506.57 ms | 8086 tok/s)\n",
      "step  956/1000 | train loss 0.731189 | norm 1.2564 | lr 1.53e-07 | (491.38 ms | 8336 tok/s)\n",
      "step  957/1000 | train loss 0.731151 | norm 1.2564 | lr 1.46e-07 | (496.16 ms | 8255 tok/s)\n",
      "step  958/1000 | train loss 0.731115 | norm 1.2562 | lr 1.40e-07 | (475.65 ms | 8611 tok/s)\n",
      "step  959/1000 | train loss 0.731080 | norm 1.2563 | lr 1.33e-07 | (522.42 ms | 7840 tok/s)\n",
      "step  960/1000 | train loss 0.731046 | norm 1.2562 | lr 1.27e-07 | (619.78 ms | 6609 tok/s)\n",
      "step  961/1000 | train loss 0.731014 | norm 1.2561 | lr 1.21e-07 | (608.49 ms | 6731 tok/s)\n",
      "step  962/1000 | train loss 0.730984 | norm 1.2560 | lr 1.15e-07 | (543.95 ms | 7530 tok/s)\n",
      "step  963/1000 | train loss 0.730955 | norm 1.2561 | lr 1.10e-07 | (545.34 ms | 7511 tok/s)\n",
      "step  964/1000 | train loss 0.730927 | norm 1.2560 | lr 1.04e-07 | (691.40 ms | 5924 tok/s)\n",
      "step  965/1000 | train loss 0.730901 | norm 1.2560 | lr 9.88e-08 | (551.29 ms | 7430 tok/s)\n",
      "step  966/1000 | train loss 0.730875 | norm 1.2559 | lr 9.36e-08 | (434.11 ms | 9435 tok/s)\n",
      "step  967/1000 | train loss 0.730851 | norm 1.2559 | lr 8.85e-08 | (465.38 ms | 8801 tok/s)\n",
      "step  968/1000 | train loss 0.730829 | norm 1.2559 | lr 8.35e-08 | (468.37 ms | 8745 tok/s)\n",
      "step  969/1000 | train loss 0.730807 | norm 1.2558 | lr 7.87e-08 | (537.97 ms | 7614 tok/s)\n",
      "step  970/1000 | train loss 0.730787 | norm 1.2558 | lr 7.41e-08 | (604.69 ms | 6774 tok/s)\n",
      "step  971/1000 | train loss 0.730768 | norm 1.2558 | lr 6.96e-08 | (559.68 ms | 7319 tok/s)\n",
      "step  972/1000 | train loss 0.730750 | norm 1.2558 | lr 6.52e-08 | (562.97 ms | 7276 tok/s)\n",
      "step  973/1000 | train loss 0.730733 | norm 1.2558 | lr 6.10e-08 | (533.14 ms | 7683 tok/s)\n",
      "step  974/1000 | train loss 0.730718 | norm 1.2557 | lr 5.69e-08 | (545.04 ms | 7515 tok/s)\n",
      "step  975/1000 | train loss 0.730703 | norm 1.2557 | lr 5.30e-08 | (556.50 ms | 7360 tok/s)\n",
      "step  976/1000 | train loss 0.730689 | norm 1.2557 | lr 4.92e-08 | (681.73 ms | 6008 tok/s)\n",
      "step  977/1000 | train loss 0.730677 | norm 1.2557 | lr 4.56e-08 | (504.71 ms | 8116 tok/s)\n",
      "step  978/1000 | train loss 0.730665 | norm 1.2557 | lr 4.21e-08 | (477.66 ms | 8575 tok/s)\n",
      "step  979/1000 | train loss 0.730654 | norm 1.2557 | lr 3.88e-08 | (495.36 ms | 8269 tok/s)\n",
      "step  980/1000 | train loss 0.730644 | norm 1.2557 | lr 3.56e-08 | (453.43 ms | 9033 tok/s)\n",
      "step  981/1000 | train loss 0.730635 | norm 1.2556 | lr 3.26e-08 | (500.43 ms | 8185 tok/s)\n",
      "step  982/1000 | train loss 0.730626 | norm 1.2556 | lr 2.97e-08 | (509.25 ms | 8043 tok/s)\n",
      "step  983/1000 | train loss 0.730619 | norm 1.2556 | lr 2.70e-08 | (569.56 ms | 7192 tok/s)\n",
      "step  984/1000 | train loss 0.730612 | norm 1.2556 | lr 2.44e-08 | (540.21 ms | 7582 tok/s)\n",
      "step  985/1000 | train loss 0.730605 | norm 1.2556 | lr 2.19e-08 | (635.78 ms | 6442 tok/s)\n",
      "step  986/1000 | train loss 0.730599 | norm 1.2556 | lr 1.97e-08 | (561.38 ms | 7296 tok/s)\n",
      "step  987/1000 | train loss 0.730594 | norm 1.2556 | lr 1.75e-08 | (536.53 ms | 7634 tok/s)\n",
      "step  988/1000 | train loss 0.730590 | norm 1.2556 | lr 1.55e-08 | (500.68 ms | 8181 tok/s)\n",
      "step  989/1000 | train loss 0.730586 | norm 1.2556 | lr 1.37e-08 | (531.66 ms | 7704 tok/s)\n",
      "step  990/1000 | train loss 0.730582 | norm 1.2556 | lr 1.20e-08 | (565.04 ms | 7249 tok/s)\n",
      "step  991/1000 | train loss 0.730579 | norm 1.2556 | lr 1.04e-08 | (548.74 ms | 7464 tok/s)\n",
      "step  992/1000 | train loss 0.730576 | norm 1.2556 | lr 8.99e-09 | (486.76 ms | 8415 tok/s)\n",
      "step  993/1000 | train loss 0.730574 | norm 1.2556 | lr 7.74e-09 | (539.01 ms | 7599 tok/s)\n",
      "step  994/1000 | train loss 0.730572 | norm 1.2556 | lr 6.63e-09 | (522.03 ms | 7846 tok/s)\n",
      "step  995/1000 | train loss 0.730570 | norm 1.2556 | lr 5.66e-09 | (607.32 ms | 6744 tok/s)\n",
      "step  996/1000 | train loss 0.730568 | norm 1.2556 | lr 4.85e-09 | (590.06 ms | 6942 tok/s)\n",
      "step  997/1000 | train loss 0.730567 | norm 1.2556 | lr 4.18e-09 | (645.68 ms | 6344 tok/s)\n",
      "step  998/1000 | train loss 0.730566 | norm 1.2556 | lr 3.67e-09 | (652.85 ms | 6274 tok/s)\n",
      "step  999/1000 | train loss 0.730565 | norm 1.2556 | lr 3.30e-09 | (510.76 ms | 8019 tok/s)\n",
      "step 1000/1000 | train loss 0.730564 | norm 1.2556 | lr 3.07e-09 | (497.59 ms | 8232 tok/s)\n",
      "step 1001/1000 | train loss 0.730563 | norm 1.2556 | lr 3.00e-09 | (475.64 ms | 8612 tok/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "timings = []\n",
    "norm = -1.0   # Normalizasyon için varsayılan değer\n",
    "for step in range(num_iterations + 1):\n",
    "    t0 = time.time()  # Başlangıç zamanı\n",
    "\n",
    "    last_step = (step == num_iterations)\n",
    "\n",
    "    # Arada bir doğrulama veri setinde değerlendirme yapılır\n",
    "    if (val_loss_every > 0 \\\n",
    "        and (step % val_loss_every == 0 or last_step)) \\\n",
    "        and (val_loader is not None):\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for _ in range(val_max_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, loss = model(x, y, return_logits=False)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= val_max_steps\n",
    "        # Konsola ve dosyaya kaydetme\n",
    "        print(f\"val loss {val_loss}\")\n",
    "\n",
    "    # Arada bir modelin örnekleme yapılması, yalnızca ana süreçte gerçekleşir\n",
    "    if (sample_every > 0 \\\n",
    "        and (step % sample_every == 0 or last_step)) \\\n",
    "        and master_process:\n",
    "        model.eval()\n",
    "        # Başlangıç için bir dizi oluşturulur, \"\" ile yeni bir dizinin başlangıcı belirtilir\n",
    "        start_ids = [63]\n",
    "        xg = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "        max_new_tokens = 32\n",
    "        temperature = 1.0\n",
    "        top_k = 40\n",
    "        yg = ham_model.generate(xg, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        print('---------------')\n",
    "        print(decode(yg[0].tolist()))\n",
    "        print('---------------')\n",
    "\n",
    "    # Modelin eğitim moduna geçmesi\n",
    "    model.train()\n",
    "\n",
    "    # Gradient birikimi yaparak istenen toplam veri boyutuna ulaşma\n",
    "    lossf = 0.0  # Gradient birikimi aşamasında ortalama kaybı almak için\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # Eğitim veri yükleyicisinden bir grup al\n",
    "        if not overfit_single_batch \\\n",
    "            or (overfit_single_batch and step == 0 and micro_step == 0):\n",
    "            x, y = train_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # İleri geçiş\n",
    "        with ctx:\n",
    "            _, loss = model(x, y, return_logits=False)\n",
    "            # Gradyan birikimini hesaba katmak için kaybı ölçeklendiriyoruz\n",
    "            loss = loss / grad_accum_steps\n",
    "            lossf += loss.detach()  # Ortalama kaybı takip etme\n",
    "        # Geriye doğru geçiş\n",
    "        if not inference_only:\n",
    "            loss.backward()\n",
    "\n",
    "    lossf = lossf.item()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    # Bu iterasyon için öğrenme oranını belirleme ve ayarlama\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Optimizer'ı güncelleme\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Eğitim bölümünün sonu\n",
    "    # Buradan sonrası sadece tanısal amaçlı, yazdırma ve günlüklere ait\n",
    "    # CPU'da tüm cihaz işlemlerinin tamamlanmasını bekleyin, böylece altındaki doğru iterasyon süreleri alabiliriz\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "    elif device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Zamanı ve çıktıları yazdırma\n",
    "    t1 = time.time()\n",
    "    tokens_per_second = grad_accum_steps * ddp_world_size * B * T / (t1-t0)\n",
    "    print(f\"step {step+1:4d}/{num_iterations} | train loss {lossf:.6f} | norm {norm:.4f} | lr {lr:.2e} | ({(t1-t0)*1000:.2f} ms | {tokens_per_second:.0f} tok/s)\")\n",
    "\n",
    "    # En son 20 iterasyonun zamanlarını takip etme\n",
    "    if step > 0 and step > num_iterations - 20:\n",
    "        timings.append(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0473,  0.0381,  0.0213, -0.0528,  0.0182],\n",
       "        [-0.0262,  0.0226,  0.0387,  0.0309,  0.0100],\n",
       "        [ 0.0157, -0.0287, -0.0171,  0.0003,  0.0004],\n",
       "        [-0.0190,  0.0157, -0.0163,  0.0365,  0.0255],\n",
       "        [-0.0175, -0.0264,  0.0066, -0.0200, -0.0431]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Eğitim öncesi\n",
    "tensor([[ 0.0385,  0.0297,  0.0180, -0.0421,  0.0136],\n",
    "        [-0.0195,  0.0192,  0.0324,  0.0290,  0.0054],\n",
    "        [ 0.0195, -0.0203, -0.0108, -0.0088, -0.0063],\n",
    "        [-0.0099,  0.0227, -0.0092,  0.0284,  0.0170],\n",
    "        [-0.0161, -0.0224,  0.0039, -0.0156, -0.0358]], device='mps:0',\n",
    "       grad_fn=<SliceBackward0>)\n",
    "\n",
    "Varmak istediğimiz yer\n",
    "tensor([[ 0.0839,  0.0716,  0.0449, -0.0960,  0.0551],\n",
    "        [-0.0607,  0.0496,  0.0738,  0.0693,  0.0313],\n",
    "        [ 0.0485, -0.0739, -0.0587, -0.0393,  0.0301],\n",
    "        [-0.0583,  0.0514, -0.0589,  0.0752,  0.0701],\n",
    "        [-0.0523, -0.0578, -0.0065, -0.0553, -0.0861]], device='mps:0',\n",
    "       grad_fn=<SliceBackward0>)\n",
    " \"\"\"\n",
    "model.lm_head.weight[:5, :5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final 20 iters avg: 551.327ms\n",
      "peak memory consumption: 0 MiB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Son 20 zaman ölçümünün ortalamasını alarak yazdırma\n",
    "timings = timings[-20:]\n",
    "print(f\"final {len(timings)} iters avg: {np.mean(timings)*1000:.3f}ms\")\n",
    "\n",
    "# Zirve bellek tüketimini yazdırma (CUDA için geçerlidir)\n",
    "print(f\"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Örnek giriş:  \n",
      "Üretilen çıktı:  dekima çığrr üze ltışamdinbiyi k\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "calistir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
